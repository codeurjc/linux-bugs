commit 6e150d605c9e21dbe939875c13e82da33fb59ed0
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Oct 7 09:16:57 2022 +0200

    Linux 5.4.217
    
    Link: https://lore.kernel.org/r/20221005113210.255710920@linuxfoundation.org
    Reviewed-by: Guenter Roeck <linux@roeck-us.net>
    Tested-by: Jon Hunter <jonathanh@nvidia.com>
    Tested-by: Linux Kernel Functional Testing <lkft@linaro.org>
    Tested-by: Slade Watkins <srw@sladewatkins.net>
    Tested-by: Allen Pais <apais@linux.microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0c41153c367be745f988ac91d9546431ddae7a29
Author: Shuah Khan <skhan@linuxfoundation.org>
Date:   Thu Sep 1 15:23:19 2022 -0600

    docs: update mediator information in CoC docs
    
    commit 8bfdfa0d6b929ede7b6189e0e546ceb6a124d05d upstream.
    
    Update mediator information in the CoC interpretation document.
    
    Signed-off-by: Shuah Khan <skhan@linuxfoundation.org>
    Link: https://lore.kernel.org/r/20220901212319.56644-1-skhan@linuxfoundation.org
    Cc: stable@vger.kernel.org
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 096740d6756019a9b9604234dd53a99d2f0effac
Author: Sami Tolvanen <samitolvanen@google.com>
Date:   Fri Sep 30 20:33:10 2022 +0000

    Makefile.extrawarn: Move -Wcast-function-type-strict to W=1
    
    commit 2120635108b35ecad9c59c8b44f6cbdf4f98214e upstream.
    
    We enable -Wcast-function-type globally in the kernel to warn about
    mismatching types in function pointer casts. Compilers currently
    warn only about ABI incompability with this flag, but Clang 16 will
    enable a stricter version of the check by default that checks for an
    exact type match. This will be very noisy in the kernel, so disable
    -Wcast-function-type-strict without W=1 until the new warnings have
    been addressed.
    
    Cc: stable@vger.kernel.org
    Link: https://reviews.llvm.org/D134831
    Link: https://github.com/ClangBuiltLinux/linux/issues/1724
    Suggested-by: Nathan Chancellor <nathan@kernel.org>
    Signed-off-by: Sami Tolvanen <samitolvanen@google.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Link: https://lore.kernel.org/r/20220930203310.4010564-1-samitolvanen@google.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e911caf9a158142cb198b88717f7450fdadd5d28
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Oct 5 12:36:45 2022 +0200

    Revert "drm/amdgpu: use dirty framebuffer helper"
    
    This reverts commit c89849ecfd2e10838b31c519c2a6607266b58f02 which is
    commit 66f99628eb24409cb8feb5061f78283c8b65f820 upstream.
    
    It is reported to cause problems on 5.4.y so it should be reverted for
    now.
    
    Reported-by: Shuah Khan <skhan@linuxfoundation.org>
    Link: https://lore.kernel.org/r/7af02bc3-c0f2-7326-e467-02549e88c9ce@linuxfoundation.org
    Cc: Hamza Mahfooz <hamza.mahfooz@amd.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Sasha Levin <sashal@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ae19c3c76dc4f4563f69bfd3d01a3c1f713cb678
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Wed Oct 5 12:31:05 2022 +0530

    xfs: remove unused variable 'done'
    
    commit b3531f5fc16d4df2b12567bce48cd9f3ab5f9131 upstream.
    
    fs/xfs/xfs_inode.c: In function 'xfs_itruncate_extents_flags':
    fs/xfs/xfs_inode.c:1523:8: warning: unused variable 'done' [-Wunused-variable]
    
    commit 4bbb04abb4ee ("xfs: truncate should remove
    all blocks, not just to the end of the page cache")
    left behind this, so remove it.
    
    Fixes: 4bbb04abb4ee ("xfs: truncate should remove all blocks, not just to the end of the page cache")
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 538657def702312cb476af380836ed910e29c3e0
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Wed Oct 5 12:31:04 2022 +0530

    xfs: fix uninitialized variable in xfs_attr3_leaf_inactive
    
    commit 54027a49938bbee1af62fad191139b14d4ee5cd2 upstream.
    
    Dan Carpenter pointed out that error is uninitialized.  While there
    never should be an attr leaf block with zero entries, let's not leave
    that logic bomb there.
    
    Fixes: 0bb9d159bd01 ("xfs: streamline xfs_attr3_leaf_inactive")
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Allison Collins <allison.henderson@oracle.com>
    Reviewed-by: Eric Sandeen <sandeen@redhat.com>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9ff41b8d71bade585976fbbf49361a8c09f41f11
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Wed Oct 5 12:31:03 2022 +0530

    xfs: streamline xfs_attr3_leaf_inactive
    
    commit 0bb9d159bd018b271e783d3b2d3bc82fa0727321 upstream.
    
    Now that we know we don't have to take a transaction to stale the incore
    buffers for a remote value, get rid of the unnecessary memory allocation
    in the leaf walker and call the rmt_stale function directly.  Flatten
    the loop while we're at it.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c893fedaf10c5688621bf804a9962e3fa0409c2a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Oct 5 12:31:02 2022 +0530

    xfs: move incore structures out of xfs_da_format.h
    
    commit a39f089a25e75c3d17b955d8eb8bc781f23364f3 upstream.
    
    Move the abstract in-memory version of various btree block headers
    out of xfs_da_format.h as they aren't on-disk formats.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5e13ad940a2a4314212b0177a3bc516dcf6dd6c9
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Wed Oct 5 12:31:01 2022 +0530

    xfs: fix memory corruption during remote attr value buffer invalidation
    
    commit e8db2aafcedb7d88320ab83f1000f1606b26d4d7 upstream.
    
    [Replaced XFS_IS_CORRUPT() calls with ASSERT() for 5.4.y backport]
    
    While running generic/103, I observed what looks like memory corruption
    and (with slub debugging turned on) a slub redzone warning on i386 when
    inactivating an inode with a 64k remote attr value.
    
    On a v5 filesystem, maximally sized remote attr values require one block
    more than 64k worth of space to hold both the remote attribute value
    header (64 bytes).  On a 4k block filesystem this results in a 68k
    buffer; on a 64k block filesystem, this would be a 128k buffer.  Note
    that even though we'll never use more than 65,600 bytes of this buffer,
    XFS_MAX_BLOCKSIZE is 64k.
    
    This is a problem because the definition of struct xfs_buf_log_format
    allows for XFS_MAX_BLOCKSIZE worth of dirty bitmap (64k).  On i386 when we
    invalidate a remote attribute, xfs_trans_binval zeroes all 68k worth of
    the dirty map, writing right off the end of the log item and corrupting
    memory.  We've gotten away with this on x86_64 for years because the
    compiler inserts a u32 padding on the end of struct xfs_buf_log_format.
    
    Fortunately for us, remote attribute values are written to disk with
    xfs_bwrite(), which is to say that they are not logged.  Fix the problem
    by removing all places where we could end up creating a buffer log item
    for a remote attribute value and leave a note explaining why.  Next,
    replace the open-coded buffer invalidation with a call to the helper we
    created in the previous patch that does better checking for bad metadata
    before marking the buffer stale.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 821e0951b4b3e16bf2e0b336f23f66c4499acc33
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Wed Oct 5 12:31:00 2022 +0530

    xfs: refactor remote attr value buffer invalidation
    
    commit 8edbb26b06023de31ad7d4c9b984d99f66577929 upstream.
    
    [Replaced XFS_IS_CORRUPT() calls with ASSERT() for 5.4.y backport]
    
    Hoist the code that invalidates remote extended attribute value buffers
    into a separate helper function.  This prepares us for a memory
    corruption fix in the next patch.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a1b66abe30dac396c897ce80b60c753852c94805
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Oct 5 12:30:59 2022 +0530

    xfs: fix IOCB_NOWAIT handling in xfs_file_dio_aio_read
    
    commit 7b53b868a1812a9a6ab5e69249394bd37f29ce2c upstream.
    
    Direct I/O reads can also be used with RWF_NOWAIT & co.  Fix the inode
    locking in xfs_file_dio_aio_read to take IOCB_NOWAIT into account.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Carlos Maiolino <cmaiolino@redhat.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1e4a0723eb384b824e89b6ddb3ff4bc0d17bdbd3
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Wed Oct 5 12:30:58 2022 +0530

    xfs: fix s_maxbytes computation on 32-bit kernels
    
    commit 932befe39ddea29cf47f4f1dc080d3dba668f0ca upstream.
    
    I observed a hang in generic/308 while running fstests on a i686 kernel.
    The hang occurred when trying to purge the pagecache on a large sparse
    file that had a page created past MAX_LFS_FILESIZE, which caused an
    integer overflow in the pagecache xarray and resulted in an infinite
    loop.
    
    I then noticed that Linus changed the definition of MAX_LFS_FILESIZE in
    commit 0cc3b0ec23ce ("Clarify (and fix) MAX_LFS_FILESIZE macros") so
    that it is now one page short of the maximum page index on 32-bit
    kernels.  Because the XFS function to compute max offset open-codes the
    2005-era MAX_LFS_FILESIZE computation and neither the vfs nor mm perform
    any sanity checking of s_maxbytes, the code in generic/308 can create a
    page above the pagecache's limit and kaboom.
    
    Fix all this by setting s_maxbytes to MAX_LFS_FILESIZE directly and
    aborting the mount with a warning if our assumptions ever break.  I have
    no answer for why this seems to have been broken for years and nobody
    noticed.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 16de74ee3ad6a3bb2aedd2b6b4fea7434ea52a7b
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Wed Oct 5 12:30:57 2022 +0530

    xfs: truncate should remove all blocks, not just to the end of the page cache
    
    commit 4bbb04abb4ee2e1f7d65e52557ba1c4038ea43ed upstream.
    
    xfs_itruncate_extents_flags() is supposed to unmap every block in a file
    from EOF onwards.  Oddly, it uses s_maxbytes as the upper limit to the
    bunmapi range, even though s_maxbytes reflects the highest offset the
    pagecache can support, not the highest offset that XFS supports.
    
    The result of this confusion is that if you create a 20T file on a
    64-bit machine, mount the filesystem on a 32-bit machine, and remove the
    file, we leak everything above 16T.  Fix this by capping the bunmapi
    request at the maximum possible block offset, not s_maxbytes.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 87e73331e4b746963c986504e19337d4a01dd81b
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Wed Oct 5 12:30:56 2022 +0530

    xfs: introduce XFS_MAX_FILEOFF
    
    commit a5084865524dee1fe8ea1fee17c60b4369ad4f5e upstream.
    
    Introduce a new #define for the maximum supported file block offset.
    We'll use this in the next patch to make it more obvious that we're
    doing some operation for all possible inode fork mappings after a given
    offset.  We can't use ULLONG_MAX here because bunmapi uses that to
    detect when it's done.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bd67d06b099dcb0b1838b07d113a285506023f70
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Oct 5 12:30:55 2022 +0530

    xfs: fix misuse of the XFS_ATTR_INCOMPLETE flag
    
    commit 780d29057781d986cd87dbbe232cd02876ad430f upstream.
    
    XFS_ATTR_INCOMPLETE is a flag in the on-disk attribute format, and thus
    in a different namespace as the ATTR_* flags in xfs_da_args.flags.
    Switch to using a XFS_DA_OP_INCOMPLETE flag in op_flags instead.  Without
    this users might be able to inject this flag into operations using the
    attr by handle ioctl.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Acked-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Chandan Babu R <chandan.babu@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 24f45c8782999f89f28e9b44178a5d409e44d9f2
Author: Daniel Sneddon <daniel.sneddon@linux.intel.com>
Date:   Mon Oct 3 10:10:38 2022 -0300

    x86/speculation: Add RSB VM Exit protections
    
    commit 2b1299322016731d56807aa49254a5ea3080b6b3 upstream.
    
    tl;dr: The Enhanced IBRS mitigation for Spectre v2 does not work as
    documented for RET instructions after VM exits. Mitigate it with a new
    one-entry RSB stuffing mechanism and a new LFENCE.
    
    == Background ==
    
    Indirect Branch Restricted Speculation (IBRS) was designed to help
    mitigate Branch Target Injection and Speculative Store Bypass, i.e.
    Spectre, attacks. IBRS prevents software run in less privileged modes
    from affecting branch prediction in more privileged modes. IBRS requires
    the MSR to be written on every privilege level change.
    
    To overcome some of the performance issues of IBRS, Enhanced IBRS was
    introduced.  eIBRS is an "always on" IBRS, in other words, just turn
    it on once instead of writing the MSR on every privilege level change.
    When eIBRS is enabled, more privileged modes should be protected from
    less privileged modes, including protecting VMMs from guests.
    
    == Problem ==
    
    Here's a simplification of how guests are run on Linux' KVM:
    
    void run_kvm_guest(void)
    {
            // Prepare to run guest
            VMRESUME();
            // Clean up after guest runs
    }
    
    The execution flow for that would look something like this to the
    processor:
    
    1. Host-side: call run_kvm_guest()
    2. Host-side: VMRESUME
    3. Guest runs, does "CALL guest_function"
    4. VM exit, host runs again
    5. Host might make some "cleanup" function calls
    6. Host-side: RET from run_kvm_guest()
    
    Now, when back on the host, there are a couple of possible scenarios of
    post-guest activity the host needs to do before executing host code:
    
    * on pre-eIBRS hardware (legacy IBRS, or nothing at all), the RSB is not
    touched and Linux has to do a 32-entry stuffing.
    
    * on eIBRS hardware, VM exit with IBRS enabled, or restoring the host
    IBRS=1 shortly after VM exit, has a documented side effect of flushing
    the RSB except in this PBRSB situation where the software needs to stuff
    the last RSB entry "by hand".
    
    IOW, with eIBRS supported, host RET instructions should no longer be
    influenced by guest behavior after the host retires a single CALL
    instruction.
    
    However, if the RET instructions are "unbalanced" with CALLs after a VM
    exit as is the RET in #6, it might speculatively use the address for the
    instruction after the CALL in #3 as an RSB prediction. This is a problem
    since the (untrusted) guest controls this address.
    
    Balanced CALL/RET instruction pairs such as in step #5 are not affected.
    
    == Solution ==
    
    The PBRSB issue affects a wide variety of Intel processors which
    support eIBRS. But not all of them need mitigation. Today,
    X86_FEATURE_RSB_VMEXIT triggers an RSB filling sequence that mitigates
    PBRSB. Systems setting RSB_VMEXIT need no further mitigation - i.e.,
    eIBRS systems which enable legacy IBRS explicitly.
    
    However, such systems (X86_FEATURE_IBRS_ENHANCED) do not set RSB_VMEXIT
    and most of them need a new mitigation.
    
    Therefore, introduce a new feature flag X86_FEATURE_RSB_VMEXIT_LITE
    which triggers a lighter-weight PBRSB mitigation versus RSB_VMEXIT.
    
    The lighter-weight mitigation performs a CALL instruction which is
    immediately followed by a speculative execution barrier (INT3). This
    steers speculative execution to the barrier -- just like a retpoline
    -- which ensures that speculation can never reach an unbalanced RET.
    Then, ensure this CALL is retired before continuing execution with an
    LFENCE.
    
    In other words, the window of exposure is opened at VM exit where RET
    behavior is troublesome. While the window is open, force RSB predictions
    sampling for RET targets to a dead end at the INT3. Close the window
    with the LFENCE.
    
    There is a subset of eIBRS systems which are not vulnerable to PBRSB.
    Add these systems to the cpu_vuln_whitelist[] as NO_EIBRS_PBRSB.
    Future systems that aren't vulnerable will set ARCH_CAP_PBRSB_NO.
    
      [ bp: Massage, incorporate review comments from Andy Cooper. ]
    
    Signed-off-by: Daniel Sneddon <daniel.sneddon@linux.intel.com>
    Co-developed-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: no intra-function validation]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 564275d4b93fb29880bee67ba76222a83a91ff26
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Mon Oct 3 10:10:37 2022 -0300

    x86/bugs: Warn when "ibrs" mitigation is selected on Enhanced IBRS parts
    
    commit eb23b5ef9131e6d65011de349a4d25ef1b3d4314 upstream.
    
    IBRS mitigation for spectre_v2 forces write to MSR_IA32_SPEC_CTRL at
    every kernel entry/exit. On Enhanced IBRS parts setting
    MSR_IA32_SPEC_CTRL[IBRS] only once at boot is sufficient. MSR writes at
    every kernel entry/exit incur unnecessary performance loss.
    
    When Enhanced IBRS feature is present, print a warning about this
    unnecessary performance loss.
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/r/2a5eaf54583c2bfe0edc4fea64006656256cca17.1657814857.git.pawan.kumar.gupta@linux.intel.com
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4891e5fd100115aa1443f5c565354a4edf83b171
Author: Nathan Chancellor <nathan@kernel.org>
Date:   Mon Oct 3 10:10:36 2022 -0300

    x86/speculation: Use DECLARE_PER_CPU for x86_spec_ctrl_current
    
    commit db886979683a8360ced9b24ab1125ad0c4d2cf76 upstream.
    
    Clang warns:
    
      arch/x86/kernel/cpu/bugs.c:58:21: error: section attribute is specified on redeclared variable [-Werror,-Wsection]
      DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
                          ^
      arch/x86/include/asm/nospec-branch.h:283:12: note: previous declaration is here
      extern u64 x86_spec_ctrl_current;
                 ^
      1 error generated.
    
    The declaration should be using DECLARE_PER_CPU instead so all
    attributes stay in sync.
    
    Cc: stable@vger.kernel.org
    Fixes: fc02735b14ff ("KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS")
    Reported-by: kernel test robot <lkp@intel.com>
    Signed-off-by: Nathan Chancellor <nathan@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9862c0f4fd6c457a591ef07ee2fd7149a67bfd73
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Mon Oct 3 10:10:35 2022 -0300

    x86/speculation: Disable RRSBA behavior
    
    commit 4ad3278df6fe2b0852b00d5757fc2ccd8e92c26e upstream.
    
    Some Intel processors may use alternate predictors for RETs on
    RSB-underflow. This condition may be vulnerable to Branch History
    Injection (BHI) and intramode-BTI.
    
    Kernel earlier added spectre_v2 mitigation modes (eIBRS+Retpolines,
    eIBRS+LFENCE, Retpolines) which protect indirect CALLs and JMPs against
    such attacks. However, on RSB-underflow, RET target prediction may
    fallback to alternate predictors. As a result, RET's predicted target
    may get influenced by branch history.
    
    A new MSR_IA32_SPEC_CTRL bit (RRSBA_DIS_S) controls this fallback
    behavior when in kernel mode. When set, RETs will not take predictions
    from alternate predictors, hence mitigating RETs as well. Support for
    this is enumerated by CPUID.7.2.EDX[RRSBA_CTRL] (bit2).
    
    For spectre v2 mitigation, when a user selects a mitigation that
    protects indirect CALLs and JMPs against BHI and intramode-BTI, set
    RRSBA_DIS_S also to protect RETs for RSB-underflow case.
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: no tools/arch/x86/include/asm/msr-index.h]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b9ae02c3c253625e6375ebb8fd30f0d3334e050b
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Mon Oct 3 10:10:34 2022 -0300

    x86/bugs: Add Cannon lake to RETBleed affected CPU list
    
    commit f54d45372c6ac9c993451de5e51312485f7d10bc upstream.
    
    Cannon lake is also affected by RETBleed, add it to the list.
    
    Fixes: 6ad0ad2bf8a6 ("x86/bugs: Report Intel retbleed vulnerability")
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d6a8a470dc22f7c5eb3affb3f1060d7ed6a5f58c
Author: Andrew Cooper <andrew.cooper3@citrix.com>
Date:   Mon Oct 3 10:10:33 2022 -0300

    x86/cpu/amd: Enumerate BTC_NO
    
    commit 26aae8ccbc1972233afd08fb3f368947c0314265 upstream.
    
    BTC_NO indicates that hardware is not susceptible to Branch Type Confusion.
    
    Zen3 CPUs don't suffer BTC.
    
    Hypervisors are expected to synthesise BTC_NO when it is appropriate
    given the migration pool, to prevent kernels using heuristics.
    
      [ bp: Massage. ]
    
    Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2edfa537f3b1dd3fbf2ff4fb70a4a408c1247693
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:32 2022 -0300

    x86/common: Stamp out the stepping madness
    
    commit 7a05bc95ed1c5a59e47aaade9fb4083c27de9e62 upstream.
    
    The whole MMIO/RETBLEED enumeration went overboard on steppings. Get
    rid of all that and simply use ANY.
    
    If a future stepping of these models would not be affected, it had
    better set the relevant ARCH_CAP_$FOO_NO bit in
    IA32_ARCH_CAPABILITIES.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 17a9fc4a7b91f8599223631bb6ae6416bc0de1c0
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:31 2022 -0300

    x86/speculation: Fill RSB on vmexit for IBRS
    
    commit 9756bba28470722dacb79ffce554336dd1f6a6cd upstream.
    
    Prevent RSB underflow/poisoning attacks with RSB.  While at it, add a
    bunch of comments to attempt to document the current state of tribal
    knowledge about RSB attacks and what exactly is being mitigated.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2242cf215013eda77838b5f3be972509aa23596e
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:30 2022 -0300

    KVM: VMX: Fix IBRS handling after vmexit
    
    commit bea7e31a5caccb6fe8ed989c065072354f0ecb52 upstream.
    
    For legacy IBRS to work, the IBRS bit needs to be always re-written
    after vmexit, even if it's already on.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 51c71ed134e97610bb0299367b390eff391f556b
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:29 2022 -0300

    KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS
    
    commit fc02735b14fff8c6678b521d324ade27b1a3d4cf upstream.
    
    On eIBRS systems, the returns in the vmexit return path from
    __vmx_vcpu_run() to vmx_vcpu_run() are exposed to RSB poisoning attacks.
    
    Fix that by moving the post-vmexit spec_ctrl handling to immediately
    after the vmexit.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a31bdec99a95c807048238df142d1784b73d8237
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Mon Oct 3 10:10:28 2022 -0300

    KVM: VMX: Convert launched argument to flags
    
    commit bb06650634d3552c0f8557e9d16aa1a408040e28 upstream.
    
    Convert __vmx_vcpu_run()'s 'launched' argument to 'flags', in
    preparation for doing SPEC_CTRL handling immediately after vmexit, which
    will need another flag.
    
    This is much easier than adding a fourth argument, because this code
    supports both 32-bit and 64-bit, and the fourth argument on 32-bit would
    have to be pushed on the stack.
    
    Note that __vmx_vcpu_run_flags() is called outside of the noinstr
    critical section because it will soon start calling potentially
    traceable functions.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5895a9297e609363ba4003c9d5839c967b237389
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:27 2022 -0300

    KVM: VMX: Flatten __vmx_vcpu_run()
    
    commit 8bd200d23ec42d66ccd517a72dd0b9cc6132d2fd upstream.
    
    Move the vmx_vm{enter,exit}() functionality into __vmx_vcpu_run().  This
    will make it easier to do the spec_ctrl handling before the first RET.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: remove ENDBR]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    [cascardo: no unwinding save/restore]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 64723cd346eaa26a5a1d4731e43385fff4b2f663
Author: Uros Bizjak <ubizjak@gmail.com>
Date:   Mon Oct 3 10:10:26 2022 -0300

    KVM/nVMX: Use __vmx_vcpu_run in nested_vmx_check_vmentry_hw
    
    commit 150f17bfab37e981ba03b37440638138ff2aa9ec upstream.
    
    Replace inline assembly in nested_vmx_check_vmentry_hw
    with a call to __vmx_vcpu_run.  The function is not
    performance critical, so (double) GPR save/restore
    in __vmx_vcpu_run can be tolerated, as far as performance
    effects are concerned.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Sean Christopherson <seanjc@google.com>
    Reviewed-and-tested-by: Sean Christopherson <seanjc@google.com>
    Signed-off-by: Uros Bizjak <ubizjak@gmail.com>
    [sean: dropped versioning info from changelog]
    Signed-off-by: Sean Christopherson <seanjc@google.com>
    Message-Id: <20201231002702.2223707-5-seanjc@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    [cascardo: small fixups]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 57ba312f10372e51c2464a8754007f0814e07a63
Author: Uros Bizjak <ubizjak@gmail.com>
Date:   Mon Oct 3 10:10:25 2022 -0300

    KVM/VMX: Use TEST %REG,%REG instead of CMP $0,%REG in vmenter.S
    
    commit 6c44221b05236cc65d76cb5dc2463f738edff39d upstream.
    
    Saves one byte in __vmx_vcpu_run for the same functionality.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Uros Bizjak <ubizjak@gmail.com>
    Message-Id: <20201029140457.126965-1-ubizjak@gmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 87dfe68a351389e7acb55be23ecc6f2614cf5563
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:24 2022 -0300

    x86/speculation: Remove x86_spec_ctrl_mask
    
    commit acac5e98ef8d638a411cfa2ee676c87e1973f126 upstream.
    
    This mask has been made redundant by kvm_spec_ctrl_test_value().  And it
    doesn't even work when MSR interception is disabled, as the guest can
    just write to SPEC_CTRL directly.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4109a8ce107def0b3642321f2d284bdd2b92976d
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:23 2022 -0300

    x86/speculation: Use cached host SPEC_CTRL value for guest entry/exit
    
    commit bbb69e8bee1bd882784947095ffb2bfe0f7c9470 upstream.
    
    There's no need to recalculate the host value for every entry/exit.
    Just use the cached value in spec_ctrl_current().
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0fd086edf8874bd77baead9e1fec065574da0107
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:22 2022 -0300

    x86/speculation: Fix SPEC_CTRL write on SMT state change
    
    commit 56aa4d221f1ee2c3a49b45b800778ec6e0ab73c5 upstream.
    
    If the SMT state changes, SSBD might get accidentally disabled.  Fix
    that.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 18d5a93fd202a6b40f14c81345f9e5e39c2ee9ee
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:21 2022 -0300

    x86/speculation: Fix firmware entry SPEC_CTRL handling
    
    commit e6aa13622ea8283cc699cac5d018cc40a2ba2010 upstream.
    
    The firmware entry code may accidentally clear STIBP or SSBD. Fix that.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 03a575a0f954450b1a89554d15bd64b5d9e861ac
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Mon Oct 3 10:10:20 2022 -0300

    x86/speculation: Fix RSB filling with CONFIG_RETPOLINE=n
    
    commit b2620facef4889fefcbf2e87284f34dcd4189bce upstream.
    
    If a kernel is built with CONFIG_RETPOLINE=n, but the user still wants
    to mitigate Spectre v2 using IBRS or eIBRS, the RSB filling will be
    silently disabled.
    
    There's nothing retpoline-specific about RSB buffer filling.  Remove the
    CONFIG_RETPOLINE guards around it.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8afd1c7da2b0484e752b28a9122efa02d284806e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:19 2022 -0300

    x86/speculation: Change FILL_RETURN_BUFFER to work with objtool
    
    commit 089dd8e53126ebaf506e2dc0bf89d652c36bfc12 upstream.
    
    Change FILL_RETURN_BUFFER so that objtool groks it and can generate
    correct ORC unwind information.
    
     - Since ORC is alternative invariant; that is, all alternatives
       should have the same ORC entries, the __FILL_RETURN_BUFFER body
       can not be part of an alternative.
    
       Therefore, move it out of the alternative and keep the alternative
       as a sort of jump_label around it.
    
     - Use the ANNOTATE_INTRA_FUNCTION_CALL annotation to white-list
       these 'funny' call instructions to nowhere.
    
     - Use UNWIND_HINT_EMPTY to 'fill' the speculation traps, otherwise
       objtool will consider them unreachable.
    
     - Move the RSP adjustment into the loop, such that the loop has a
       deterministic stack layout.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Link: https://lkml.kernel.org/r/20200428191700.032079304@infradead.org
    [cascardo: fixup because of backport of ba6e31af2be96c4d0536f2152ed6f7b6c11bca47 ("x86/speculation: Add LFENCE to RSB fill sequence")]
    [cascardo: no intra-function call validation support]
    [cascardo: avoid UNWIND_HINT_EMPTY because of svm]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3ee9e9a5af0711bf0848cddb070aae72777318c6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:18 2022 -0300

    intel_idle: Disable IBRS during long idle
    
    commit bf5835bcdb9635c97f85120dba9bfa21e111130f upstream.
    
    Having IBRS enabled while the SMT sibling is idle unnecessarily slows
    down the running sibling. OTOH, disabling IBRS around idle takes two
    MSR writes, which will increase the idle latency.
    
    Therefore, only disable IBRS around deeper idle states. Shallow idle
    states are bounded by the tick in duration, since NOHZ is not allowed
    for them by virtue of their short target residency.
    
    Only do this for mwait-driven idle, since that keeps interrupts disabled
    across idle, which makes disabling IBRS vs IRQ-entry a non-issue.
    
    Note: C6 is a random threshold, most importantly C1 probably shouldn't
    disable IBRS, benchmarking needed.
    
    Suggested-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: no CPUIDLE_FLAG_IRQ_ENABLE]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    [cascardo: context adjustments]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 97bc52c14a9372260d5d9bc0272129b29b3eda39
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:17 2022 -0300

    x86/bugs: Report Intel retbleed vulnerability
    
    commit 6ad0ad2bf8a67e27d1f9d006a1dabb0e1c360cc3 upstream.
    
    Skylake suffers from RSB underflow speculation issues; report this
    vulnerability and it's mitigation (spectre_v2=ibrs).
    
      [jpoimboe: cleanups, eibrs]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fd67fe3db93f931eae24733db72dc82e85bc0bfe
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:16 2022 -0300

    x86/bugs: Split spectre_v2_select_mitigation() and spectre_v2_user_select_mitigation()
    
    commit 166115c08a9b0b846b783088808a27d739be6e8d upstream.
    
    retbleed will depend on spectre_v2, while spectre_v2_user depends on
    retbleed. Break this cycle.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2d4ce2d72c3b46f20929fa7d1c8af5d5250ccfc7
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Mon Oct 3 10:10:15 2022 -0300

    x86/speculation: Add spectre_v2=ibrs option to support Kernel IBRS
    
    commit 7c693f54c873691a4b7da05c7e0f74e67745d144 upstream.
    
    Extend spectre_v2= boot option with Kernel IBRS.
    
      [jpoimboe: no STIBP with IBRS]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e2d793a3742a2502533ee586a3f9151670e3f467
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:14 2022 -0300

    x86/bugs: Optimize SPEC_CTRL MSR writes
    
    commit c779bc1a9002fa474175b80e72b85c9bf628abb0 upstream.
    
    When changing SPEC_CTRL for user control, the WRMSR can be delayed
    until return-to-user when KERNEL_IBRS has been enabled.
    
    This avoids an MSR write during context switch.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a3111faed5c1db1b008f2eb498cc823ebf162eed
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:13 2022 -0300

    x86/entry: Add kernel IBRS implementation
    
    commit 2dbb887e875b1de3ca8f40ddf26bcfe55798c609 upstream.
    
    Implement Kernel IBRS - currently the only known option to mitigate RSB
    underflow speculation issues on Skylake hardware.
    
    Note: since IBRS_ENTER requires fuller context established than
    UNTRAIN_RET, it must be placed after it. However, since UNTRAIN_RET
    itself implies a RET, it must come after IBRS_ENTER. This means
    IBRS_ENTER needs to also move UNTRAIN_RET.
    
    Note 2: KERNEL_IBRS is sub-optimal for XenPV.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: conflict at arch/x86/entry/entry_64.S, skip_r11rcx]
    [cascardo: conflict at arch/x86/entry/entry_64_compat.S]
    [cascardo: conflict fixups, no ANNOTATE_NOENDBR]
    [cascardo: entry fixups because of missing UNTRAIN_RET]
    [cascardo: conflicts on fsgsbase]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fd32a31553a158bbafd59bcdd82a5b26996e850f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:12 2022 -0300

    x86/entry: Remove skip_r11rcx
    
    commit 1b331eeea7b8676fc5dbdf80d0a07e41be226177 upstream.
    
    Yes, r11 and rcx have been restored previously, but since they're being
    popped anyway (into rsi) might as well pop them into their own regs --
    setting them to the value they already are.
    
    Less magical code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20220506121631.365070674@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3c93ff4e23ea4000355a53445123e677e3ebc70a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:11 2022 -0300

    x86/bugs: Keep a per-CPU IA32_SPEC_CTRL value
    
    commit caa0ff24d5d0e02abce5e65c3d2b7f20a6617be5 upstream.
    
    Due to TIF_SSBD and TIF_SPEC_IB the actual IA32_SPEC_CTRL value can
    differ from x86_spec_ctrl_base. As such, keep a per-CPU value
    reflecting the current task's MSR content.
    
      [jpoimboe: rename]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9a596426d7bd6404ec378e1bcb2e5295ebaf7272
Author: Alexandre Chartre <alexandre.chartre@oracle.com>
Date:   Mon Oct 3 10:10:10 2022 -0300

    x86/bugs: Add AMD retbleed= boot parameter
    
    commit 7fbf47c7ce50b38a64576b150e7011ae73d54669 upstream.
    
    Add the "retbleed=<value>" boot parameter to select a mitigation for
    RETBleed. Possible values are "off", "auto" and "unret"
    (JMP2RET mitigation). The default value is "auto".
    
    Currently, "retbleed=auto" will select the unret mitigation on
    AMD and Hygon and no mitigation on Intel (JMP2RET is not effective on
    Intel).
    
      [peterz: rebase; add hygon]
      [jpoimboe: cleanups]
    
    Signed-off-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: this effectively remove the UNRET mitigation as an option, so it
     has to be complemented by a later pick of the same commit later. This is
     done in order to pick retbleed_select_mitigation]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 063b7f980607ac1420cea73971d4bba90f629518
Author: Alexandre Chartre <alexandre.chartre@oracle.com>
Date:   Mon Oct 3 10:10:09 2022 -0300

    x86/bugs: Report AMD retbleed vulnerability
    
    commit 6b80b59b3555706508008f1f127b5412c89c7fd8 upstream.
    
    Report that AMD x86 CPUs are vulnerable to the RETBleed (Arbitrary
    Speculative Code Execution with Return Instructions) attack.
    
      [peterz: add hygon]
      [kim: invert parity; fam15h]
    
    Co-developed-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: adjusted BUG numbers to match upstream]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 954d591a84d0dbadc98ef0567f57ce686b8fc2ee
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:08 2022 -0300

    x86/cpufeatures: Move RETPOLINE flags to word 11
    
    commit a883d624aed463c84c22596006e5a96f5b44db31 upstream.
    
    In order to extend the RETPOLINE features to 4, move them to word 11
    where there is still room. This mostly keeps DISABLE_RETPOLINE
    simple.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 893cd858b09ca20c8c919db8dc5b009895626da3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 3 10:10:07 2022 -0300

    x86/kvm/vmx: Make noinstr clean
    
    commit 742ab6df974ae8384a2dd213db1a3a06cf6d8936 upstream.
    
    The recent mmio_stale_data fixes broke the noinstr constraints:
    
      vmlinux.o: warning: objtool: vmx_vcpu_enter_exit+0x15b: call to wrmsrl.constprop.0() leaves .noinstr.text section
      vmlinux.o: warning: objtool: vmx_vcpu_enter_exit+0x1bf: call to kvm_arch_has_assigned_device() leaves .noinstr.text section
    
    make it all happy again.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f62d272c2fec959de89287183361e65ef900e7ca
Author: Mark Gross <mgross@linux.intel.com>
Date:   Mon Oct 3 10:10:06 2022 -0300

    x86/cpu: Add a steppings field to struct x86_cpu_id
    
    commit e9d7144597b10ff13ff2264c059f7d4a7fbc89ac upstream.
    
    Intel uses the same family/model for several CPUs. Sometimes the
    stepping must be checked to tell them apart.
    
    On x86 there can be at most 16 steppings. Add a steppings bitmask to
    x86_cpu_id and a X86_MATCH_VENDOR_FAMILY_MODEL_STEPPING_FEATURE macro
    and support for matching against family/model/stepping.
    
     [ bp: Massage. ]
    
    Signed-off-by: Mark Gross <mgross@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    [cascardo: have steppings be the last member as there are initializers
     that don't use named members]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 69460b1ed63d91b892ca551818933498c733b023
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 3 10:10:05 2022 -0300

    x86/cpu: Add consistent CPU match macros
    
    commit 20d437447c0089cda46c683db219d3b4e2cde40e upstream.
    
    Finding all places which build x86_cpu_id match tables is tedious and the
    logic is hidden in lots of differently named macro wrappers.
    
    Most of these initializer macros use plain C89 initializers which rely on
    the ordering of the struct members. So new members could only be added at
    the end of the struct, but that's ugly as hell and C99 initializers are
    really the right thing to use.
    
    Provide a set of macros which:
    
      - Have a proper naming scheme, starting with X86_MATCH_
    
      - Use C99 initializers
    
    The set of provided macros are all subsets of the base macro
    
        X86_MATCH_VENDOR_FAM_MODEL_FEATURE()
    
    which allows to supply all possible selection criteria:
    
          vendor, family, model, feature
    
    The other macros shorten this to avoid typing all arguments when they are
    not needed and would require one of the _ANY constants. They have been
    created due to the requirements of the existing usage sites.
    
    Also add a few model constants for Centaur CPUs and QUARK.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Link: https://lkml.kernel.org/r/20200320131508.826011988@linutronix.de
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 87449d94e75c2596d5f2e1f2878ddfc150af23ad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 3 10:10:04 2022 -0300

    x86/devicetable: Move x86 specific macro out of generic code
    
    commit ba5bade4cc0d2013cdf5634dae554693c968a090 upstream.
    
    There is no reason that this gunk is in a generic header file. The wildcard
    defines need to stay as they are required by file2alias.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Link: https://lkml.kernel.org/r/20200320131508.736205164@linutronix.de
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fbd29b7549b281ef5777f3ba3df3ab96fa3102b7
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Mon Oct 3 10:10:03 2022 -0300

    Revert "x86/cpu: Add a steppings field to struct x86_cpu_id"
    
    This reverts commit 749ec6b48a9a41f95154cd5aa61053aaeb7c7aff.
    
    This is commit e9d7144597b10ff13ff2264c059f7d4a7fbc89ac upstream. A proper
    backport will be done. This will make it easier to check for parts affected
    by Retbleed, which require X86_MATCH_VENDOR_FAM_MODEL.
    
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3a8ff61e6f136a11543d4210092bfd8879598720
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Mon Oct 3 10:10:02 2022 -0300

    Revert "x86/speculation: Add RSB VM Exit protections"
    
    This reverts commit f2f41ef0352db9679bfae250d7a44b3113f3a3cc.
    
    This is commit 2b1299322016731d56807aa49254a5ea3080b6b3 upstream.
    
    In order to apply IBRS mitigation for Retbleed, PBRSB mitigations must be
    reverted and the reapplied, so the backports can look sane.
    
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
