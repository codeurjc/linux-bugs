commit a9e2d8e52e1c0d87c0fa4f9d2d38e210aabed515
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Sat Jul 23 12:54:14 2022 +0200

    Linux 5.15.57
    
    Link: https://lore.kernel.org/r/20220722091133.320803732@linuxfoundation.org
    Tested-by: Bagas Sanjaya <bagasdotme@gmail.com>
    Tested-by: Florian Fainelli <f.fainelli@gmail.com>
    Tested-by: Linux Kernel Functional Testing <lkft@linaro.org>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Tested-by: Ron Economos <re@w6rz.net>
    Tested-by: Sudip Mukherjee <sudip.mukherjee@codethink.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b4296a7b863f201f9be08fee6f8891f4912de011
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 19 17:50:25 2021 +0100

    x86: Use -mindirect-branch-cs-prefix for RETPOLINE builds
    
    commit 68cf4f2a72ef8786e6b7af6fd9a89f27ac0f520d upstream.
    
    In order to further enable commit:
    
      bbe2df3f6b6d ("x86/alternative: Try inline spectre_v2=retpoline,amd")
    
    add the new GCC flag -mindirect-branch-cs-prefix:
    
      https://gcc.gnu.org/g:2196a681d7810ad8b227bf983f38ba716620545e
      https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102952
      https://bugs.llvm.org/show_bug.cgi?id=52323
    
    to RETPOLINE=y builds. This should allow fully inlining retpoline,amd
    for GCC builds.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Nick Desaulniers <ndesaulniers@google.com>
    Link: https://lkml.kernel.org/r/20211119165630.276205624@infradead.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 06741ef8e7cced9729e8061778bc97df58cd4649
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jul 14 12:20:19 2022 +0200

    um: Add missing apply_returns()
    
    commit 564d998106397394b6aad260f219b882b3347e62 upstream.
    
    Implement apply_returns() stub for UM, just like all the other patching
    routines.
    
    Fixes: 15e67227c49a ("x86: Undo return-thunk damage")
    Reported-by: Randy Dunlap <rdunlap@infradead.org)
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/Ys%2Ft45l%2FgarIrD0u@worktop.programming.kicks-ass.net
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 320fc994f0c88d81dc684e346a7e115fa8364137
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Fri Jul 8 16:21:28 2022 -0500

    x86/bugs: Remove apostrophe typo
    
    commit bcf163150cd37348a0cb59e95c916a83a9344b0e upstream.
    
    Remove a superfluous ' in the mitigation string.
    
    Fixes: e8ec1b6e08a2 ("x86/bugs: Enable STIBP for JMP2RET")
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ac8edadc2b1e96caca7270dc14640c4e424f4280
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Jul 1 13:39:15 2021 -0300

    tools headers cpufeatures: Sync with the kernel sources
    
    commit f098addbdb44c8a565367f5162f3ab170ed9404a upstream.
    
    To pick the changes from:
    
      f43b9876e857c739 ("x86/retbleed: Add fine grained Kconfig knobs")
      a149180fbcf336e9 ("x86: Add magic AMD return-thunk")
      15e67227c49a5783 ("x86: Undo return-thunk damage")
      369ae6ffc41a3c11 ("x86/retpoline: Cleanup some #ifdefery")
      4ad3278df6fe2b08 x86/speculation: Disable RRSBA behavior
      26aae8ccbc197223 x86/cpu/amd: Enumerate BTC_NO
      9756bba28470722d x86/speculation: Fill RSB on vmexit for IBRS
      3ebc170068885b6f x86/bugs: Add retbleed=ibpb
      2dbb887e875b1de3 x86/entry: Add kernel IBRS implementation
      6b80b59b35557065 x86/bugs: Report AMD retbleed vulnerability
      a149180fbcf336e9 x86: Add magic AMD return-thunk
      15e67227c49a5783 x86: Undo return-thunk damage
      a883d624aed463c8 x86/cpufeatures: Move RETPOLINE flags to word 11
      51802186158c74a0 x86/speculation/mmio: Enumerate Processor MMIO Stale Data bug
    
    This only causes these perf files to be rebuilt:
    
      CC       /tmp/build/perf/bench/mem-memcpy-x86-64-asm.o
      CC       /tmp/build/perf/bench/mem-memset-x86-64-asm.o
    
    And addresses this perf build warning:
    
      Warning: Kernel ABI header at 'tools/arch/x86/include/asm/cpufeatures.h' differs from latest version at 'arch/x86/include/asm/cpufeatures.h'
      diff -u tools/arch/x86/include/asm/cpufeatures.h arch/x86/include/asm/cpufeatures.h
      Warning: Kernel ABI header at 'tools/arch/x86/include/asm/disabled-features.h' differs from latest version at 'arch/x86/include/asm/disabled-features.h'
      diff -u tools/arch/x86/include/asm/disabled-features.h arch/x86/include/asm/disabled-features.h
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Ian Rogers <irogers@google.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org
    Link: https://lore.kernel.org/lkml/YtQM40VmiLTkPND2@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1dd6c13b437c0d7c997c16fcb73afdf67e625c33
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Jul 1 13:32:18 2021 -0300

    tools arch x86: Sync the msr-index.h copy with the kernel sources
    
    commit 91d248c3b903b46a58cbc7e8d38d684d3e4007c2 upstream.
    
    To pick up the changes from these csets:
    
      4ad3278df6fe2b08 ("x86/speculation: Disable RRSBA behavior")
      d7caac991feeef1b ("x86/cpu/amd: Add Spectral Chicken")
    
    That cause no changes to tooling:
    
      $ tools/perf/trace/beauty/tracepoints/x86_msr.sh > before
      $ cp arch/x86/include/asm/msr-index.h tools/arch/x86/include/asm/msr-index.h
      $ tools/perf/trace/beauty/tracepoints/x86_msr.sh > after
      $ diff -u before after
      $
    
    Just silences this perf build warning:
    
      Warning: Kernel ABI header at 'tools/arch/x86/include/asm/msr-index.h' differs from latest version at 'arch/x86/include/asm/msr-index.h'
      diff -u tools/arch/x86/include/asm/msr-index.h arch/x86/include/asm/msr-index.h
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Ian Rogers <irogers@google.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/lkml/YtQTm9wsB3hxQWvy@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5f4e77cc9abf65c4593cd255d8c3cd6c5d0ca7f6
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jul 15 07:34:55 2022 -0400

    KVM: emulate: do not adjust size of fastop and setcc subroutines
    
    commit 79629181607e801c0b41b8790ac4ee2eb5d7bc3e upstream.
    
    Instead of doing complicated calculations to find the size of the subroutines
    (which are even more complicated because they need to be stringified into
    an asm statement), just hardcode to 16.
    
    It is less dense for a few combinations of IBT/SLS/retbleed, but it has
    the advantage of being really simple.
    
    Cc: stable@vger.kernel.org # 5.15.x: 84e7051c0bc1: x86/kvm: fix FASTOP_SIZE when return thunks are enabled
    Cc: stable@vger.kernel.org
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 198685e58b134679e6079d872396f4352780282f
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Wed Jul 13 14:12:41 2022 -0300

    x86/kvm: fix FASTOP_SIZE when return thunks are enabled
    
    commit 84e7051c0bc1f2a13101553959b3a9d9a8e24939 upstream.
    
    The return thunk call makes the fastop functions larger, just like IBT
    does. Consider a 16-byte FASTOP_SIZE when CONFIG_RETHUNK is enabled.
    
    Otherwise, functions will be incorrectly aligned and when computing their
    position for differently sized operators, they will executed in the middle
    or end of a function, which may as well be an int3, leading to a crash
    like:
    
    [   36.091116] int3: 0000 [#1] SMP NOPTI
    [   36.091119] CPU: 3 PID: 1371 Comm: qemu-system-x86 Not tainted 5.15.0-41-generic #44
    [   36.091120] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.15.0-1 04/01/2014
    [   36.091121] RIP: 0010:xaddw_ax_dx+0x9/0x10 [kvm]
    [   36.091185] Code: 00 0f bb d0 c3 cc cc cc cc 48 0f bb d0 c3 cc cc cc cc 0f 1f 80 00 00 00 00 0f c0 d0 c3 cc cc cc cc 66 0f c1 d0 c3 cc cc cc cc <0f> 1f 80 00 00 00 00 0f c1 d0 c3 cc cc cc cc 48 0f c1 d0 c3 cc cc
    [   36.091186] RSP: 0018:ffffb1f541143c98 EFLAGS: 00000202
    [   36.091188] RAX: 0000000089abcdef RBX: 0000000000000001 RCX: 0000000000000000
    [   36.091188] RDX: 0000000076543210 RSI: ffffffffc073c6d0 RDI: 0000000000000200
    [   36.091189] RBP: ffffb1f541143ca0 R08: ffff9f1803350a70 R09: 0000000000000002
    [   36.091190] R10: ffff9f1803350a70 R11: 0000000000000000 R12: ffff9f1803350a70
    [   36.091190] R13: ffffffffc077fee0 R14: 0000000000000000 R15: 0000000000000000
    [   36.091191] FS:  00007efdfce8d640(0000) GS:ffff9f187dd80000(0000) knlGS:0000000000000000
    [   36.091192] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   36.091192] CR2: 0000000000000000 CR3: 0000000009b62002 CR4: 0000000000772ee0
    [   36.091195] PKRU: 55555554
    [   36.091195] Call Trace:
    [   36.091197]  <TASK>
    [   36.091198]  ? fastop+0x5a/0xa0 [kvm]
    [   36.091222]  x86_emulate_insn+0x7b8/0xe90 [kvm]
    [   36.091244]  x86_emulate_instruction+0x2f4/0x630 [kvm]
    [   36.091263]  ? kvm_arch_vcpu_load+0x7c/0x230 [kvm]
    [   36.091283]  ? vmx_prepare_switch_to_host+0xf7/0x190 [kvm_intel]
    [   36.091290]  complete_emulated_mmio+0x297/0x320 [kvm]
    [   36.091310]  kvm_arch_vcpu_ioctl_run+0x32f/0x550 [kvm]
    [   36.091330]  kvm_vcpu_ioctl+0x29e/0x6d0 [kvm]
    [   36.091344]  ? kvm_vcpu_ioctl+0x120/0x6d0 [kvm]
    [   36.091357]  ? __fget_files+0x86/0xc0
    [   36.091362]  ? __fget_files+0x86/0xc0
    [   36.091363]  __x64_sys_ioctl+0x92/0xd0
    [   36.091366]  do_syscall_64+0x59/0xc0
    [   36.091369]  ? syscall_exit_to_user_mode+0x27/0x50
    [   36.091370]  ? do_syscall_64+0x69/0xc0
    [   36.091371]  ? syscall_exit_to_user_mode+0x27/0x50
    [   36.091372]  ? __x64_sys_writev+0x1c/0x30
    [   36.091374]  ? do_syscall_64+0x69/0xc0
    [   36.091374]  ? exit_to_user_mode_prepare+0x37/0xb0
    [   36.091378]  ? syscall_exit_to_user_mode+0x27/0x50
    [   36.091379]  ? do_syscall_64+0x69/0xc0
    [   36.091379]  ? do_syscall_64+0x69/0xc0
    [   36.091380]  ? do_syscall_64+0x69/0xc0
    [   36.091381]  ? do_syscall_64+0x69/0xc0
    [   36.091381]  entry_SYSCALL_64_after_hwframe+0x61/0xcb
    [   36.091384] RIP: 0033:0x7efdfe6d1aff
    [   36.091390] Code: 00 48 89 44 24 18 31 c0 48 8d 44 24 60 c7 04 24 10 00 00 00 48 89 44 24 08 48 8d 44 24 20 48 89 44 24 10 b8 10 00 00 00 0f 05 <41> 89 c0 3d 00 f0 ff ff 77 1f 48 8b 44 24 18 64 48 2b 04 25 28 00
    [   36.091391] RSP: 002b:00007efdfce8c460 EFLAGS: 00000246 ORIG_RAX: 0000000000000010
    [   36.091393] RAX: ffffffffffffffda RBX: 000000000000ae80 RCX: 00007efdfe6d1aff
    [   36.091393] RDX: 0000000000000000 RSI: 000000000000ae80 RDI: 000000000000000c
    [   36.091394] RBP: 0000558f1609e220 R08: 0000558f13fb8190 R09: 00000000ffffffff
    [   36.091394] R10: 0000558f16b5e950 R11: 0000000000000246 R12: 0000000000000000
    [   36.091394] R13: 0000000000000001 R14: 0000000000000000 R15: 0000000000000000
    [   36.091396]  </TASK>
    [   36.091397] Modules linked in: isofs nls_iso8859_1 kvm_intel joydev kvm input_leds serio_raw sch_fq_codel dm_multipath scsi_dh_rdac scsi_dh_emc scsi_dh_alua ipmi_devintf ipmi_msghandler drm msr ip_tables x_tables autofs4 btrfs blake2b_generic zstd_compress raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel virtio_net net_failover crypto_simd ahci xhci_pci cryptd psmouse virtio_blk libahci xhci_pci_renesas failover
    [   36.123271] ---[ end trace db3c0ab5a48fabcc ]---
    [   36.123272] RIP: 0010:xaddw_ax_dx+0x9/0x10 [kvm]
    [   36.123319] Code: 00 0f bb d0 c3 cc cc cc cc 48 0f bb d0 c3 cc cc cc cc 0f 1f 80 00 00 00 00 0f c0 d0 c3 cc cc cc cc 66 0f c1 d0 c3 cc cc cc cc <0f> 1f 80 00 00 00 00 0f c1 d0 c3 cc cc cc cc 48 0f c1 d0 c3 cc cc
    [   36.123320] RSP: 0018:ffffb1f541143c98 EFLAGS: 00000202
    [   36.123321] RAX: 0000000089abcdef RBX: 0000000000000001 RCX: 0000000000000000
    [   36.123321] RDX: 0000000076543210 RSI: ffffffffc073c6d0 RDI: 0000000000000200
    [   36.123322] RBP: ffffb1f541143ca0 R08: ffff9f1803350a70 R09: 0000000000000002
    [   36.123322] R10: ffff9f1803350a70 R11: 0000000000000000 R12: ffff9f1803350a70
    [   36.123323] R13: ffffffffc077fee0 R14: 0000000000000000 R15: 0000000000000000
    [   36.123323] FS:  00007efdfce8d640(0000) GS:ffff9f187dd80000(0000) knlGS:0000000000000000
    [   36.123324] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   36.123325] CR2: 0000000000000000 CR3: 0000000009b62002 CR4: 0000000000772ee0
    [   36.123327] PKRU: 55555554
    [   36.123328] Kernel panic - not syncing: Fatal exception in interrupt
    [   36.123410] Kernel Offset: 0x1400000 from 0xffffffff81000000 (relocation range: 0xffffffff80000000-0xffffffffbfffffff)
    [   36.135305] ---[ end Kernel panic - not syncing: Fatal exception in interrupt ]---
    
    Fixes: aa3d480315ba ("x86: Use return-thunk in asm code")
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Co-developed-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Josh Poimboeuf <jpoimboe@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Reported-by: Linux Kernel Functional Testing <lkft@linaro.org>
    Message-Id: <20220713171241.184026-1-cascardo@canonical.com>
    Tested-by: Jack Wang <jinpu.wang@ionos.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 554cf28b104d6f4a215304bbaae4bf7713b0ea0d
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Fri Jul 15 16:45:50 2022 -0300

    efi/x86: use naked RET on mixed mode call wrapper
    
    commit 51a6fa0732d6be6a44e0032752ad2ac10d67c796 upstream.
    
    When running with return thunks enabled under 32-bit EFI, the system
    crashes with:
    
      kernel tried to execute NX-protected page - exploit attempt? (uid: 0)
      BUG: unable to handle page fault for address: 000000005bc02900
      #PF: supervisor instruction fetch in kernel mode
      #PF: error_code(0x0011) - permissions violation
      PGD 18f7063 P4D 18f7063 PUD 18ff063 PMD 190e063 PTE 800000005bc02063
      Oops: 0011 [#1] PREEMPT SMP PTI
      CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.19.0-rc6+ #166
      Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 0.0.0 02/06/2015
      RIP: 0010:0x5bc02900
      Code: Unable to access opcode bytes at RIP 0x5bc028d6.
      RSP: 0018:ffffffffb3203e10 EFLAGS: 00010046
      RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000048
      RDX: 000000000190dfac RSI: 0000000000001710 RDI: 000000007eae823b
      RBP: ffffffffb3203e70 R08: 0000000001970000 R09: ffffffffb3203e28
      R10: 747563657865206c R11: 6c6977203a696665 R12: 0000000000001710
      R13: 0000000000000030 R14: 0000000001970000 R15: 0000000000000001
      FS:  0000000000000000(0000) GS:ffff8e013ca00000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0018 ES: 0018 CR0: 0000000080050033
      CR2: 000000005bc02900 CR3: 0000000001930000 CR4: 00000000000006f0
      Call Trace:
       ? efi_set_virtual_address_map+0x9c/0x175
       efi_enter_virtual_mode+0x4a6/0x53e
       start_kernel+0x67c/0x71e
       x86_64_start_reservations+0x24/0x2a
       x86_64_start_kernel+0xe9/0xf4
       secondary_startup_64_no_verify+0xe5/0xeb
    
    That's because it cannot jump to the return thunk from the 32-bit code.
    
    Using a naked RET and marking it as safe allows the system to proceed
    booting.
    
    Fixes: aa3d480315ba ("x86: Use return-thunk in asm code")
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Josh Poimboeuf <jpoimboe@kernel.org>
    Cc: <stable@vger.kernel.org>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d0caa861a836fe4d85749e8a8d16623bb91b39d3
Author: Nathan Chancellor <nathan@kernel.org>
Date:   Wed Jul 13 08:24:37 2022 -0700

    x86/speculation: Use DECLARE_PER_CPU for x86_spec_ctrl_current
    
    commit db886979683a8360ced9b24ab1125ad0c4d2cf76 upstream.
    
    Clang warns:
    
      arch/x86/kernel/cpu/bugs.c:58:21: error: section attribute is specified on redeclared variable [-Werror,-Wsection]
      DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
                          ^
      arch/x86/include/asm/nospec-branch.h:283:12: note: previous declaration is here
      extern u64 x86_spec_ctrl_current;
                 ^
      1 error generated.
    
    The declaration should be using DECLARE_PER_CPU instead so all
    attributes stay in sync.
    
    Cc: stable@vger.kernel.org
    Fixes: fc02735b14ff ("KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS")
    Reported-by: kernel test robot <lkp@intel.com>
    Signed-off-by: Nathan Chancellor <nathan@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 67040d1d922be2bb58e693032067ffa97aac46ad
Author: Jiri Slaby <jirislaby@kernel.org>
Date:   Wed Jul 13 11:50:46 2022 +0200

    x86/asm/32: Fix ANNOTATE_UNRET_SAFE use on 32-bit
    
    commit 3131ef39fb03bbde237d0b8260445898f3dfda5b upstream.
    
    The build on x86_32 currently fails after commit
    
      9bb2ec608a20 (objtool: Update Retpoline validation)
    
    with:
    
      arch/x86/kernel/../../x86/xen/xen-head.S:35: Error: no such instruction: `annotate_unret_safe'
    
    ANNOTATE_UNRET_SAFE is defined in nospec-branch.h. And head_32.S is
    missing this include. Fix this.
    
    Fixes: 9bb2ec608a20 ("objtool: Update Retpoline validation")
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/63e23f80-033f-f64e-7522-2816debbc367@kernel.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 96907c5d3927515029ad1f38a31d56dc04b8d288
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Thu Jul 14 00:39:33 2022 +0200

    x86/xen: Fix initialisation in hypercall_page after rethunk
    
    The hypercall_page is special and the RETs there should not be changed
    into rethunk calls (but can have SLS mitigation).  Change the initial
    instructions to ret + int3 padding, as was done in upstream commit
    5b2fc51576ef "x86/ibt,xen: Sprinkle the ENDBR".
    
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 86ccf19a91beed4c1d1a2c9dc9c7af1f991a7fea
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 12 14:01:06 2022 +0200

    x86/static_call: Serialize __static_call_fixup() properly
    
    commit c27c753ea6fd1237f4f96abf8b623d7bab505513 upstream.
    
    __static_call_fixup() invokes __static_call_transform() without holding
    text_mutex, which causes lockdep to complain in text_poke_bp().
    
    Adding the proper locking cures that, but as this is either used during
    early boot or during module finalizing, it's not required to use
    text_poke_bp(). Add an argument to __static_call_transform() which tells
    it to use text_poke_early() for it.
    
    Fixes: ee88d363d156 ("x86,static_call: Use alternative RET encoding")
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 73ad137d1146eabd8c82496a47477006dce3b91d
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Fri Jul 8 13:36:09 2022 -0700

    x86/speculation: Disable RRSBA behavior
    
    commit 4ad3278df6fe2b0852b00d5757fc2ccd8e92c26e upstream.
    
    Some Intel processors may use alternate predictors for RETs on
    RSB-underflow. This condition may be vulnerable to Branch History
    Injection (BHI) and intramode-BTI.
    
    Kernel earlier added spectre_v2 mitigation modes (eIBRS+Retpolines,
    eIBRS+LFENCE, Retpolines) which protect indirect CALLs and JMPs against
    such attacks. However, on RSB-underflow, RET target prediction may
    fallback to alternate predictors. As a result, RET's predicted target
    may get influenced by branch history.
    
    A new MSR_IA32_SPEC_CTRL bit (RRSBA_DIS_S) controls this fallback
    behavior when in kernel mode. When set, RETs will not take predictions
    from alternate predictors, hence mitigating RETs as well. Support for
    this is enumerated by CPUID.7.2.EDX[RRSBA_CTRL] (bit2).
    
    For spectre v2 mitigation, when a user selects a mitigation that
    protects indirect CALLs and JMPs against BHI and intramode-BTI, set
    RRSBA_DIS_S also to protect RETs for RSB-underflow case.
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: no X86_FEATURE_INTEL_PPIN]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit db0128b8243fe6f1004b07a622ab8f3bc7260be8
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Jul 8 19:10:11 2022 +0200

    x86/kexec: Disable RET on kexec
    
    commit 697977d8415d61f3acbc4ee6d564c9dcf0309507 upstream.
    
    All the invocations unroll to __x86_return_thunk and this file
    must be PIC independent.
    
    This fixes kexec on 64-bit AMD boxes.
    
      [ bp: Fix 32-bit build. ]
    
    Reported-by: Edward Tran <edward.tran@oracle.com>
    Reported-by: Awais Tanveer <awais.tanveer@oracle.com>
    Suggested-by: Ankur Arora <ankur.a.arora@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3653093b7bbfa8c8c61858bffdc7f1e41ad436e1
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Thu Jul 7 13:41:52 2022 -0300

    x86/bugs: Do not enable IBPB-on-entry when IBPB is not supported
    
    commit 2259da159fbe5dba8ac00b560cf00b6a6537fa18 upstream.
    
    There are some VM configurations which have Skylake model but do not
    support IBPB. In those cases, when using retbleed=ibpb, userspace is going
    to be killed and kernel is going to panic.
    
    If the CPU does not support IBPB, warn and proceed with the auto option. Also,
    do not fallback to IBPB on AMD/Hygon systems if it is not supported.
    
    Fixes: 3ebc17006888 ("x86/bugs: Add retbleed=ibpb")
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 925340f99bdfd747bcb6950d5576224fd75dc103
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jul 6 15:33:30 2022 +0200

    x86/entry: Move PUSH_AND_CLEAR_REGS() back into error_entry
    
    commit 2c08b9b38f5b0f4a6c2d29be22b695e4ec4a556b upstream.
    
    Commit
    
      ee774dac0da1 ("x86/entry: Move PUSH_AND_CLEAR_REGS out of error_entry()")
    
    moved PUSH_AND_CLEAR_REGS out of error_entry, into its own function, in
    part to avoid calling error_entry() for XenPV.
    
    However, commit
    
      7c81c0c9210c ("x86/entry: Avoid very early RET")
    
    had to change that because the 'ret' was too early and moved it into
    idtentry, bloating the text size, since idtentry is expanded for every
    exception vector.
    
    However, with the advent of xen_error_entry() in commit
    
      d147553b64bad ("x86/xen: Add UNTRAIN_RET")
    
    it became possible to remove PUSH_AND_CLEAR_REGS from idtentry, back
    into *error_entry().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: error_entry still does cld]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2f8967e22322251f3c4aa74f6ee30c267d5f91bb
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Wed Jul 6 15:01:15 2022 -0700

    x86/bugs: Add Cannon lake to RETBleed affected CPU list
    
    commit f54d45372c6ac9c993451de5e51312485f7d10bc upstream.
    
    Cannon lake is also affected by RETBleed, add it to the list.
    
    Fixes: 6ad0ad2bf8a6 ("x86/bugs: Report Intel retbleed vulnerability")
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cc3011cdbe5f834b0e9873b841d1be02be9a1524
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jun 27 22:21:17 2022 +0000

    x86/retbleed: Add fine grained Kconfig knobs
    
    commit f43b9876e857c739d407bc56df288b0ebe1a9164 upstream.
    
    Do fine-grained Kconfig for all the various retbleed parts.
    
    NOTE: if your compiler doesn't support return thunks this will
    silently 'upgrade' your mitigation to IBPB, you might not like this.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: there is no CONFIG_OBJTOOL]
    [cascardo: objtool calling and option parsing has changed]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 12a13570054fcbf347aaf37824ea43915f9f739c
Author: Andrew Cooper <andrew.cooper3@citrix.com>
Date:   Fri Jun 24 14:41:21 2022 +0100

    x86/cpu/amd: Enumerate BTC_NO
    
    commit 26aae8ccbc1972233afd08fb3f368947c0314265 upstream.
    
    BTC_NO indicates that hardware is not susceptible to Branch Type Confusion.
    
    Zen3 CPUs don't suffer BTC.
    
    Hypervisors are expected to synthesise BTC_NO when it is appropriate
    given the migration pool, to prevent kernels using heuristics.
    
      [ bp: Massage. ]
    
    Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: no X86_FEATURE_BRS]
    [cascardo: no X86_FEATURE_CPPC]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d7a5c08b46b66d6fef6bad800ea5a4bfbaefc3f7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jun 24 14:03:25 2022 +0200

    x86/common: Stamp out the stepping madness
    
    commit 7a05bc95ed1c5a59e47aaade9fb4083c27de9e62 upstream.
    
    The whole MMIO/RETBLEED enumeration went overboard on steppings. Get
    rid of all that and simply use ANY.
    
    If a future stepping of these models would not be affected, it had
    better set the relevant ARCH_CAP_$FOO_NO bit in
    IA32_ARCH_CAPABILITIES.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8d5cff499a6d740c91ff37963907e0e983c37f0f
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:15 2022 +0200

    x86/speculation: Fill RSB on vmexit for IBRS
    
    commit 9756bba28470722dacb79ffce554336dd1f6a6cd upstream.
    
    Prevent RSB underflow/poisoning attacks with RSB.  While at it, add a
    bunch of comments to attempt to document the current state of tribal
    knowledge about RSB attacks and what exactly is being mitigated.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4207d7b645b82b9a2c0faffeb9f36999ac040a2a
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:14 2022 +0200

    KVM: VMX: Fix IBRS handling after vmexit
    
    commit bea7e31a5caccb6fe8ed989c065072354f0ecb52 upstream.
    
    For legacy IBRS to work, the IBRS bit needs to be always re-written
    after vmexit, even if it's already on.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0cbd5905c8f3b5da498c21c4deee347622b3420b
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:13 2022 +0200

    KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS
    
    commit fc02735b14fff8c6678b521d324ade27b1a3d4cf upstream.
    
    On eIBRS systems, the returns in the vmexit return path from
    __vmx_vcpu_run() to vmx_vcpu_run() are exposed to RSB poisoning attacks.
    
    Fix that by moving the post-vmexit spec_ctrl handling to immediately
    after the vmexit.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5fde25284dfe9d3f12afdceec410cddb8d4b889a
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:12 2022 +0200

    KVM: VMX: Convert launched argument to flags
    
    commit bb06650634d3552c0f8557e9d16aa1a408040e28 upstream.
    
    Convert __vmx_vcpu_run()'s 'launched' argument to 'flags', in
    preparation for doing SPEC_CTRL handling immediately after vmexit, which
    will need another flag.
    
    This is much easier than adding a fourth argument, because this code
    supports both 32-bit and 64-bit, and the fourth argument on 32-bit would
    have to be pushed on the stack.
    
    Note that __vmx_vcpu_run_flags() is called outside of the noinstr
    critical section because it will soon start calling potentially
    traceable functions.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 852fe53040b973db272270a4a1ee1be0b51e112c
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:11 2022 +0200

    KVM: VMX: Flatten __vmx_vcpu_run()
    
    commit 8bd200d23ec42d66ccd517a72dd0b9cc6132d2fd upstream.
    
    Move the vmx_vm{enter,exit}() functionality into __vmx_vcpu_run().  This
    will make it easier to do the spec_ctrl handling before the first RET.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: remove ENDBR]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c590fa2d2dac50e9e1ea6946506636617b2f8694
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Fri Jun 24 12:52:40 2022 +0200

    objtool: Re-add UNWIND_HINT_{SAVE_RESTORE}
    
    commit 8faea26e611189e933ea2281975ff4dc7c1106b6 upstream.
    
    Commit
    
      c536ed2fffd5 ("objtool: Remove SAVE/RESTORE hints")
    
    removed the save/restore unwind hints because they were no longer
    needed. Now they're going to be needed again so re-add them.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 66b37dccbc2524b675bfb2b3f4281a943cc36169
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Fri Jun 17 12:12:48 2022 -0700

    x86/speculation: Remove x86_spec_ctrl_mask
    
    commit acac5e98ef8d638a411cfa2ee676c87e1973f126 upstream.
    
    This mask has been made redundant by kvm_spec_ctrl_test_value().  And it
    doesn't even work when MSR interception is disabled, as the guest can
    just write to SPEC_CTRL directly.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b55663e7521fe8257ad722206e5296f19b7ff0d7
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:08 2022 +0200

    x86/speculation: Use cached host SPEC_CTRL value for guest entry/exit
    
    commit bbb69e8bee1bd882784947095ffb2bfe0f7c9470 upstream.
    
    There's no need to recalculate the host value for every entry/exit.
    Just use the cached value in spec_ctrl_current().
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2069bd0e6f5b0c4ded790ff34b8ee5df909b51e0
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:07 2022 +0200

    x86/speculation: Fix SPEC_CTRL write on SMT state change
    
    commit 56aa4d221f1ee2c3a49b45b800778ec6e0ab73c5 upstream.
    
    If the SMT state changes, SSBD might get accidentally disabled.  Fix
    that.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e898e28762f830592d723ddb34db8a86735a2e70
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:06 2022 +0200

    x86/speculation: Fix firmware entry SPEC_CTRL handling
    
    commit e6aa13622ea8283cc699cac5d018cc40a2ba2010 upstream.
    
    The firmware entry code may accidentally clear STIBP or SSBD. Fix that.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d427c1f83e8cab913c52d047b891ee895f989e61
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 23:16:05 2022 +0200

    x86/speculation: Fix RSB filling with CONFIG_RETPOLINE=n
    
    commit b2620facef4889fefcbf2e87284f34dcd4189bce upstream.
    
    If a kernel is built with CONFIG_RETPOLINE=n, but the user still wants
    to mitigate Spectre v2 using IBRS or eIBRS, the RSB filling will be
    silently disabled.
    
    There's nothing retpoline-specific about RSB buffer filling.  Remove the
    CONFIG_RETPOLINE guards around it.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 75d4fc2e965f89f5844637ead257f03ec6f1d269
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:16:04 2022 +0200

    x86/cpu/amd: Add Spectral Chicken
    
    commit d7caac991feeef1b871ee6988fd2c9725df09039 upstream.
    
    Zen2 uarchs have an undocumented, unnamed, MSR that contains a chicken
    bit for some speculation behaviour. It needs setting.
    
    Note: very belatedly AMD released naming; it's now officially called
          MSR_AMD64_DE_CFG2 and MSR_AMD64_DE_CFG2_SUPPRESS_NOBR_PRED_BIT
          but shall remain the SPECTRAL CHICKEN.
    
    Suggested-by: Andrew Cooper <Andrew.Cooper3@citrix.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit dab72c3c32ccde4f362fff993759b0133448d394
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:16:03 2022 +0200

    objtool: Add entry UNRET validation
    
    commit a09a6e2399ba0595c3042b3164f3ca68a3cff33e upstream.
    
    Since entry asm is tricky, add a validation pass that ensures the
    retbleed mitigation has been done before the first actual RET
    instruction.
    
    Entry points are those that either have UNWIND_HINT_ENTRY, which acts
    as UNWIND_HINT_EMPTY but marks the instruction as an entry point, or
    those that have UWIND_HINT_IRET_REGS at +0.
    
    This is basically a variant of validate_branch() that is
    intra-function and it will simply follow all branches from marked
    entry points and ensures that all paths lead to ANNOTATE_UNRET_END.
    
    If a path hits RET or an indirection the path is a fail and will be
    reported.
    
    There are 3 ANNOTATE_UNRET_END instances:
    
     - UNTRAIN_RET itself
     - exception from-kernel; this path doesn't need UNTRAIN_RET
     - all early exceptions; these also don't need UNTRAIN_RET
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: tools/objtool/builtin-check.c no link option validation]
    [cascardo: tools/objtool/check.c opts.ibt is ibt]
    [cascardo: tools/objtool/include/objtool/builtin.h leave unret option as bool, no struct opts]
    [cascardo: objtool is still called from scripts/link-vmlinux.sh]
    [cascardo: no IBT support]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 469d9b1570177275983f2f987be279f43bfb1c42
Author: Josh Poimboeuf <jpoimboe@kernel.org>
Date:   Tue Jun 14 15:07:19 2022 -0700

    x86/bugs: Do IBPB fallback check only once
    
    commit 0fe4aeea9c01baabecc8c3afc7889c809d939bc2 upstream.
    
    When booting with retbleed=auto, if the kernel wasn't built with
    CONFIG_CC_HAS_RETURN_THUNK, the mitigation falls back to IBPB.  Make
    sure a warning is printed in that case.  The IBPB fallback check is done
    twice, but it really only needs to be done once.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fd17a4254965fc44044b3d7f5be186880ed7a021
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:16:02 2022 +0200

    x86/bugs: Add retbleed=ibpb
    
    commit 3ebc170068885b6fc7bedda6c667bb2c4d533159 upstream.
    
    jmp2ret mitigates the easy-to-attack case at relatively low overhead.
    It mitigates the long speculation windows after a mispredicted RET, but
    it does not mitigate the short speculation window from arbitrary
    instruction boundaries.
    
    On Zen2, there is a chicken bit which needs setting, which mitigates
    "arbitrary instruction boundaries" down to just "basic block boundaries".
    
    But there is no fix for the short speculation window on basic block
    boundaries, other than to flush the entire BTB to evict all attacker
    predictions.
    
    On the spectrum of "fast & blurry" -> "safe", there is (on top of STIBP
    or no-SMT):
    
      1) Nothing            System wide open
      2) jmp2ret            May stop a script kiddy
      3) jmp2ret+chickenbit  Raises the bar rather further
      4) IBPB               Only thing which can count as "safe".
    
    Tentative numbers put IBPB-on-entry at a 2.5x hit on Zen2, and a 10x hit
    on Zen1 according to lmbench.
    
      [ bp: Fixup feature bit comments, document option, 32-bit build fix. ]
    
    Suggested-by: Andrew Cooper <Andrew.Cooper3@citrix.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8894f699f187dd1bb2e62b683b15d064ba5fb583
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:16:01 2022 +0200

    x86/xen: Add UNTRAIN_RET
    
    commit d147553b64bad34d2f92cb7d8ba454ae95c3baac upstream.
    
    Ensure the Xen entry also passes through UNTRAIN_RET.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 87e6270183a4062b46c21bf483539eea7854df6a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:16:00 2022 +0200

    x86/xen: Rename SYS* entry points
    
    commit b75b7f8ef1148be1b9321ffc2f6c19238904b438 upstream.
    
    Native SYS{CALL,ENTER} entry points are called
    entry_SYS{CALL,ENTER}_{64,compat}, make sure the Xen versions are
    named consistently.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e894b7817e08a588f6046f3f7e20c05dd9c6e16f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:59 2022 +0200

    objtool: Update Retpoline validation
    
    commit 9bb2ec608a209018080ca262f771e6a9ff203b6f upstream.
    
    Update retpoline validation with the new CONFIG_RETPOLINE requirement of
    not having bare naked RET instructions.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: conflict fixup at arch/x86/xen/xen-head.S]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a1d912d7ac04d6a010c2b3a637a244e088d7f5bc
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:58 2022 +0200

    intel_idle: Disable IBRS during long idle
    
    commit bf5835bcdb9635c97f85120dba9bfa21e111130f upstream.
    
    Having IBRS enabled while the SMT sibling is idle unnecessarily slows
    down the running sibling. OTOH, disabling IBRS around idle takes two
    MSR writes, which will increase the idle latency.
    
    Therefore, only disable IBRS around deeper idle states. Shallow idle
    states are bounded by the tick in duration, since NOHZ is not allowed
    for them by virtue of their short target residency.
    
    Only do this for mwait-driven idle, since that keeps interrupts disabled
    across idle, which makes disabling IBRS vs IRQ-entry a non-issue.
    
    Note: C6 is a random threshold, most importantly C1 probably shouldn't
    disable IBRS, benchmarking needed.
    
    Suggested-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: no CPUIDLE_FLAG_IRQ_ENABLE]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e51769df6228c57a33dc2546951b7e18c08d1b07
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jun 24 13:48:58 2022 +0200

    x86/bugs: Report Intel retbleed vulnerability
    
    commit 6ad0ad2bf8a67e27d1f9d006a1dabb0e1c360cc3 upstream.
    
    Skylake suffers from RSB underflow speculation issues; report this
    vulnerability and it's mitigation (spectre_v2=ibrs).
    
      [jpoimboe: cleanups, eibrs]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b9349805e65380c3de8fff90a05472e131ce96f8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:56 2022 +0200

    x86/bugs: Split spectre_v2_select_mitigation() and spectre_v2_user_select_mitigation()
    
    commit 166115c08a9b0b846b783088808a27d739be6e8d upstream.
    
    retbleed will depend on spectre_v2, while spectre_v2_user depends on
    retbleed. Break this cycle.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 347d0bf6b0c020ca0284b3fd96652470e06bd084
Author: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
Date:   Tue Jun 14 23:15:55 2022 +0200

    x86/speculation: Add spectre_v2=ibrs option to support Kernel IBRS
    
    commit 7c693f54c873691a4b7da05c7e0f74e67745d144 upstream.
    
    Extend spectre_v2= boot option with Kernel IBRS.
    
      [jpoimboe: no STIBP with IBRS]
    
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e03415eeba781f0b7568d359e5f161a7b3a5c7c7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:54 2022 +0200

    x86/bugs: Optimize SPEC_CTRL MSR writes
    
    commit c779bc1a9002fa474175b80e72b85c9bf628abb0 upstream.
    
    When changing SPEC_CTRL for user control, the WRMSR can be delayed
    until return-to-user when KERNEL_IBRS has been enabled.
    
    This avoids an MSR write during context switch.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 62b4db57eefec6ab544749a4f0178c4f68bad835
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Sat Jul 9 23:42:53 2022 -0300

    x86/entry: Add kernel IBRS implementation
    
    commit 2dbb887e875b1de3ca8f40ddf26bcfe55798c609 upstream.
    
    Implement Kernel IBRS - currently the only known option to mitigate RSB
    underflow speculation issues on Skylake hardware.
    
    Note: since IBRS_ENTER requires fuller context established than
    UNTRAIN_RET, it must be placed after it. However, since UNTRAIN_RET
    itself implies a RET, it must come after IBRS_ENTER. This means
    IBRS_ENTER needs to also move UNTRAIN_RET.
    
    Note 2: KERNEL_IBRS is sub-optimal for XenPV.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: conflict at arch/x86/entry/entry_64_compat.S]
    [cascardo: conflict fixups, no ANNOTATE_NOENDBR]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fac1b0007cec8335f47b01358f352a9bdd72f7fb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:52 2022 +0200

    x86/bugs: Keep a per-CPU IA32_SPEC_CTRL value
    
    commit caa0ff24d5d0e02abce5e65c3d2b7f20a6617be5 upstream.
    
    Due to TIF_SSBD and TIF_SPEC_IB the actual IA32_SPEC_CTRL value can
    differ from x86_spec_ctrl_base. As such, keep a per-CPU value
    reflecting the current task's MSR content.
    
      [jpoimboe: rename]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b4e05ea71edaf52f2ecef675407a82160176ac54
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Tue Jun 14 23:15:51 2022 +0200

    x86/bugs: Enable STIBP for JMP2RET
    
    commit e8ec1b6e08a2102d8755ccb06fa26d540f26a2fa upstream.
    
    For untrained return thunks to be fully effective, STIBP must be enabled
    or SMT disabled.
    
    Co-developed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 89eba42632fcfff30e84f66a5a7ca63882ac64ea
Author: Alexandre Chartre <alexandre.chartre@oracle.com>
Date:   Tue Jun 14 23:15:50 2022 +0200

    x86/bugs: Add AMD retbleed= boot parameter
    
    commit 7fbf47c7ce50b38a64576b150e7011ae73d54669 upstream.
    
    Add the "retbleed=<value>" boot parameter to select a mitigation for
    RETBleed. Possible values are "off", "auto" and "unret"
    (JMP2RET mitigation). The default value is "auto".
    
    Currently, "retbleed=auto" will select the unret mitigation on
    AMD and Hygon and no mitigation on Intel (JMP2RET is not effective on
    Intel).
    
      [peterz: rebase; add hygon]
      [jpoimboe: cleanups]
    
    Signed-off-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 82e92fe936dbefda25ed030abe91ae76576cd6da
Author: Alexandre Chartre <alexandre.chartre@oracle.com>
Date:   Tue Jun 14 23:15:49 2022 +0200

    x86/bugs: Report AMD retbleed vulnerability
    
    commit 6b80b59b3555706508008f1f127b5412c89c7fd8 upstream.
    
    Report that AMD x86 CPUs are vulnerable to the RETBleed (Arbitrary
    Speculative Code Execution with Return Instructions) attack.
    
      [peterz: add hygon]
      [kim: invert parity; fam15h]
    
    Co-developed-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a9c0926fc75468ccea0cf1f3b48b0d5a41291975
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:48 2022 +0200

    x86: Add magic AMD return-thunk
    
    commit a149180fbcf336e97ce4eb2cdc13672727feb94d upstream.
    
    Note: needs to be in a section distinct from Retpolines such that the
    Retpoline RET substitution cannot possibly use immediate jumps.
    
    ORC unwinding for zen_untrain_ret() and __x86_return_thunk() is a
    little tricky but works due to the fact that zen_untrain_ret() doesn't
    have any stack ops and as such will emit a single ORC entry at the
    start (+0x3f).
    
    Meanwhile, unwinding an IP, including the __x86_return_thunk() one
    (+0x40) will search for the largest ORC entry smaller or equal to the
    IP, these will find the one ORC entry (+0x3f) and all works.
    
      [ Alexandre: SVM part. ]
      [ bp: Build fix, massages. ]
    
    Suggested-by: Andrew Cooper <Andrew.Cooper3@citrix.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: conflicts at arch/x86/entry/entry_64_compat.S]
    [cascardo: there is no ANNOTATE_NOENDBR]
    [cascardo: objtool commit 34c861e806478ac2ea4032721defbf1d6967df08 missing]
    [cascardo: conflict fixup]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 07f5c5e36236368ca761868ed82c8b654f58968a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:47 2022 +0200

    objtool: Treat .text.__x86.* as noinstr
    
    commit 951ddecf435659553ed15a9214e153a3af43a9a1 upstream.
    
    Needed because zen_untrain_ret() will be called from noinstr code.
    
    Also makes sense since the thunks MUST NOT contain instrumentation nor
    be poked with dynamic instrumentation.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1f068f9da7436b11276f23ebf65dd641c8ae0b62
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:46 2022 +0200

    x86/entry: Avoid very early RET
    
    commit 7c81c0c9210c9bfab2bae76aab2999de5bad27db upstream.
    
    Commit
    
      ee774dac0da1 ("x86/entry: Move PUSH_AND_CLEAR_REGS out of error_entry()")
    
    manages to introduce a CALL/RET pair that is before SWITCH_TO_KERNEL_CR3,
    which means it is before RETBleed can be mitigated.
    
    Revert to an earlier version of the commit in Fixes. Down side is that
    this will bloat .text size somewhat. The alternative is fully reverting
    it.
    
    The purpose of this patch was to allow migrating error_entry() to C,
    including the whole of kPTI. Much care needs to be taken moving that
    forward to not re-introduce this problem of early RETs.
    
    Fixes: ee774dac0da1 ("x86/entry: Move PUSH_AND_CLEAR_REGS out of error_entry()")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1d61a2988612ac0632134454d5407c63ae0b9d42
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:45 2022 +0200

    x86: Use return-thunk in asm code
    
    commit aa3d480315ba6c3025a60958e1981072ea37c3df upstream.
    
    Use the return thunk in asm code. If the thunk isn't needed, it will
    get patched into a RET instruction during boot by apply_returns().
    
    Since alternatives can't handle relocations outside of the first
    instruction, putting a 'jmp __x86_return_thunk' in one is not valid,
    therefore carve out the memmove ERMS path into a separate label and jump
    to it.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: no RANDSTRUCT_CFLAGS]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6c45176bc2ae4c3c2417202903f19cd86b9d7651
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Tue Jun 14 23:15:44 2022 +0200

    x86/sev: Avoid using __x86_return_thunk
    
    commit 0ee9073000e8791f8b134a8ded31bcc767f7f232 upstream.
    
    Specifically, it's because __enc_copy() encrypts the kernel after
    being relocated outside the kernel in sme_encrypt_execute(), and the
    RET macro's jmp offset isn't amended prior to execution.
    
    Signed-off-by: Kim Phillips <kim.phillips@amd.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f3d9f990586c8f85ed2ced627f15801d39a7ed87
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:43 2022 +0200

    x86/vsyscall_emu/64: Don't use RET in vsyscall emulation
    
    commit 15583e514eb16744b80be85dea0774ece153177d upstream.
    
    This is userspace code and doesn't play by the normal kernel rules.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5b43965d58efb6ef15cd7988cb15116fb154ad54
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:42 2022 +0200

    x86/kvm: Fix SETcc emulation for return thunks
    
    commit af2e140f34208a5dfb6b7a8ad2d56bda88f0524d upstream.
    
    Prepare the SETcc fastop stuff for when RET can be larger still.
    
    The tricky bit here is that the expressions should not only be
    constant C expressions, but also absolute GAS expressions. This means
    no ?: and 'true' is ~0.
    
    Also ensure em_setcc() has the same alignment as the actual FOP_SETCC()
    ops, this ensures there cannot be an alignment hole between em_setcc()
    and the first op.
    
    Additionally, add a .skip directive to the FOP_SETCC() macro to fill
    any remaining space with INT3 traps; however the primary purpose of
    this directive is to generate AS warnings when the remaining space
    goes negative. Which is a very good indication the alignment magic
    went side-ways.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: ignore ENDBR when computing SETCC_LENGTH]
    [cascardo: conflict fixup]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0bfe8be824aadc24da2963978a6c6e743f1e4f46
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:41 2022 +0200

    x86/bpf: Use alternative RET encoding
    
    commit d77cfe594ad50e0bf95d457e02ccd578791b2a15 upstream.
    
    Use the return thunk in eBPF generated code, if needed.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e54fcb0812faebd147de72bd37ad87cc4951c68c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:40 2022 +0200

    x86/ftrace: Use alternative RET encoding
    
    commit 1f001e9da6bbf482311e45e48f53c2bd2179e59c upstream.
    
    Use the return thunk in ftrace trampolines, if needed.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: still copy return from ftrace_stub]
    [cascardo: use memcpy(text_gen_insn) as there is no __text_gen_insn]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c1c80aabc7b968ccae5c134b4e4a30739e32efda
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:39 2022 +0200

    x86,static_call: Use alternative RET encoding
    
    commit ee88d363d15617ff50ac24fab0ffec11113b2aeb upstream.
    
    In addition to teaching static_call about the new way to spell 'RET',
    there is an added complication in that static_call() is allowed to
    rewrite text before it is known which particular spelling is required.
    
    In order to deal with this; have a static_call specific fixup in the
    apply_return() 'alternative' patching routine that will rewrite the
    static_call trampoline to match the definite sequence.
    
    This in turn creates the problem of uniquely identifying static call
    trampolines. Currently trampolines are 8 bytes, the first 5 being the
    jmp.d32/ret sequence and the final 3 a byte sequence that spells out
    'SCT'.
    
    This sequence is used in __static_call_validate() to ensure it is
    patching a trampoline and not a random other jmp.d32. That is,
    false-positives shouldn't be plenty, but aren't a big concern.
    
    OTOH the new __static_call_fixup() must not have false-positives, and
    'SCT' decodes to the somewhat weird but semi plausible sequence:
    
      push %rbx
      rex.XB push %r12
    
    Additionally, there are SLS concerns with immediate jumps. Combined it
    seems like a good moment to change the signature to a single 3 byte
    trap instruction that is unique to this usage and will not ever get
    generated by accident.
    
    As such, change the signature to: '0x0f, 0xb9, 0xcc', which decodes
    to:
    
      ud1 %esp, %ecx
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: skip validation as introduced by 2105a92748e8 ("static_call,x86: Robustify trampoline patching")]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b0fb9784cf803472bbdcd3d98364c27966fee679
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Fri Jul 1 09:00:45 2022 -0300

    objtool: skip non-text sections when adding return-thunk sites
    
    The .discard.text section is added in order to reserve BRK, with a
    temporary function just so it can give it a size. This adds a relocation to
    the return thunk, which objtool will add to the .return_sites section.
    Linking will then fail as there are references to the .discard.text
    section.
    
    Do not add instructions from non-text sections to the list of return thunk
    calls, avoiding the reference to .discard.text.
    
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Acked-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1920e4be8a975f769a9c2d2e77a1f1a02c88b58a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:38 2022 +0200

    x86,objtool: Create .return_sites
    
    commit d9e9d2300681d68a775c28de6aa6e5290ae17796 upstream.
    
    Find all the return-thunk sites and record them in a .return_sites
    section such that the kernel can undo this.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: conflict fixup because of functions added to support IBT]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7bf553d9eebd6bdf70e10a2fc3a7fa6dd482347e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:37 2022 +0200

    x86: Undo return-thunk damage
    
    commit 15e67227c49a57837108acfe1c80570e1bd9f962 upstream.
    
    Introduce X86_FEATURE_RETHUNK for those afflicted with needing this.
    
      [ bp: Do only INT3 padding - simpler. ]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: CONFIG_STACK_VALIDATION vs CONFIG_OBJTOOL]
    [cascardo: no IBT support]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 023a2b07d5c3436427d7b902d7cddc16469027ba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:36 2022 +0200

    x86/retpoline: Use -mfunction-return
    
    commit 0b53c374b9eff2255a386f1f1cfb9a928e52a5ae upstream.
    
    Utilize -mfunction-return=thunk-extern when available to have the
    compiler replace RET instructions with direct JMPs to the symbol
    __x86_return_thunk. This does not affect assembler (.S) sources, only C
    sources.
    
    -mfunction-return=thunk-extern has been available since gcc 7.3 and
    clang 15.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Tested-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: RETPOLINE_CFLAGS is at Makefile]
    [cascardo: remove ANNOTATE_NOENDBR from __x86_return_thunk]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bb81f3ac69f854c99444a55f2b4735ba8875732a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:35 2022 +0200

    x86/retpoline: Swizzle retpoline thunk
    
    commit 00e1533325fd1fb5459229fe37f235462649f668 upstream.
    
    Put the actual retpoline thunk as the original code so that it can
    become more complicated. Specifically, it allows RET to be a JMP,
    which can't be .altinstr_replacement since that doesn't do relocations
    (except for the very first instruction).
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 545b45c36a2f5f217d059478f29cefa9f1afe26c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:34 2022 +0200

    x86/retpoline: Cleanup some #ifdefery
    
    commit 369ae6ffc41a3c1137cab697635a84d0cc7cdcea upstream.
    
    On it's own not much of a cleanup but it prepares for more/similar
    code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    [cascardo: conflict fixup because of DISABLE_ENQCMD]
    [cascardo: no changes at nospec-branch.h and bpf_jit_comp.c]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit eee4f31fa2ebc5eb88f3b7320b909dc580c7de47
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:33 2022 +0200

    x86/cpufeatures: Move RETPOLINE flags to word 11
    
    commit a883d624aed463c84c22596006e5a96f5b44db31 upstream.
    
    In order to extend the RETPOLINE features to 4, move them to word 11
    where there is still room. This mostly keeps DISABLE_RETPOLINE
    simple.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ccb25d7db1a29bc251692be745b000e6f0754048
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 14 23:15:32 2022 +0200

    x86/kvm/vmx: Make noinstr clean
    
    commit 742ab6df974ae8384a2dd213db1a3a06cf6d8936 upstream.
    
    The recent mmio_stale_data fixes broke the noinstr constraints:
    
      vmlinux.o: warning: objtool: vmx_vcpu_enter_exit+0x15b: call to wrmsrl.constprop.0() leaves .noinstr.text section
      vmlinux.o: warning: objtool: vmx_vcpu_enter_exit+0x1bf: call to kvm_arch_has_assigned_device() leaves .noinstr.text section
    
    make it all happy again.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2cd972ae337f4ff40b31513b324ff846a0247437
Author: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
Date:   Fri Jul 1 11:21:20 2022 -0300

    x86/realmode: build with -D__DISABLE_EXPORTS
    
    Commit 156ff4a544ae ("x86/ibt: Base IBT bits") added this option when
    building realmode in order to disable IBT there. This is also needed in
    order to disable return thunks.
    
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6834878ace6e09c332f4c2bc5df2fb33a9e82126
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 6 14:14:35 2022 +0200

    x86/entry: Remove skip_r11rcx
    
    commit 1b331eeea7b8676fc5dbdf80d0a07e41be226177 upstream.
    
    Yes, r11 and rcx have been restored previously, but since they're being
    popped anyway (into rsi) might as well pop them into their own regs --
    setting them to the value they already are.
    
    Less magical code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20220506121631.365070674@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 40265bcd1bd3a00e13aa51330aa2b5a4b6aaf338
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Mar 8 16:30:14 2022 +0100

    objtool: Default ignore INT3 for unreachable
    
    commit 1ffbe4e935f9b7308615c75be990aec07464d1e7 upstream.
    
    Ignore all INT3 instructions for unreachable code warnings, similar to NOP.
    This allows using INT3 for various paddings instead of NOPs.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Link: https://lore.kernel.org/r/20220308154317.343312938@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 32b2cd6060e7ab2984e750d0fbe79e1655cc7cbe
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:48 2021 +0200

    bpf,x86: Respect X86_FEATURE_RETPOLINE*
    
    commit 87c87ecd00c54ecd677798cb49ef27329e0fab41 upstream.
    
    Current BPF codegen doesn't respect X86_FEATURE_RETPOLINE* flags and
    unconditionally emits a thunk call, this is sub-optimal and doesn't
    match the regular, compiler generated, code.
    
    Update the i386 JIT to emit code equal to what the compiler emits for
    the regular kernel text (IOW. a plain THUNK call).
    
    Update the x86_64 JIT to emit code similar to the result of compiler
    and kernel rewrites as according to X86_FEATURE_RETPOLINE* flags.
    Inlining RETPOLINE_AMD (lfence; jmp *%reg) and !RETPOLINE (jmp *%reg),
    while doing a THUNK call for RETPOLINE.
    
    This removes the hard-coded retpoline thunks and shrinks the generated
    code. Leaving a single retpoline thunk definition in the kernel.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.614772675@infradead.org
    [cascardo: RETPOLINE_AMD was renamed to RETPOLINE_LFENCE]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 74b5a9f8edfeb7458653a673fa49a7c505acdf02
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:47 2021 +0200

    bpf,x86: Simplify computing label offsets
    
    commit dceba0817ca329868a15e2e1dd46eb6340b69206 upstream.
    
    Take an idea from the 32bit JIT, which uses the multi-pass nature of
    the JIT to compute the instruction offsets on a prior pass in order to
    compute the relative jump offsets on a later pass.
    
    Application to the x86_64 JIT is slightly more involved because the
    offsets depend on program variables (such as callee_regs_used and
    stack_depth) and hence the computed offsets need to be kept in the
    context of the JIT.
    
    This removes, IMO quite fragile, code that hard-codes the offsets and
    tries to compute the length of variable parts of it.
    
    Convert both emit_bpf_tail_call_*() functions which have an out: label
    at the end. Additionally emit_bpt_tail_call_direct() also has a poke
    table entry, for which it computes the offset from the end (and thus
    already relies on the previous pass to have computed addrs[i]), also
    convert this to be a forward based offset.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.552304864@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1c2e7b40752a7a3290bcf3c498d73353a02323ad
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:45 2021 +0200

    x86/alternative: Add debug prints to apply_retpolines()
    
    commit d4b5a5c993009ffeb5febe3b701da3faab6adb96 upstream.
    
    Make sure we can see the text changes when booting with
    'debug-alternative'.
    
    Example output:
    
     [ ] SMP alternatives: retpoline at: __traceiter_initcall_level+0x1f/0x30 (ffffffff8100066f) len: 5 to: __x86_indirect_thunk_rax+0x0/0x20
     [ ] SMP alternatives: ffffffff82603e58: [2:5) optimized NOPs: ff d0 0f 1f 00
     [ ] SMP alternatives: ffffffff8100066f: orig: e8 cc 30 00 01
     [ ] SMP alternatives: ffffffff8100066f: repl: ff d0 0f 1f 00
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.422273830@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b93a0a740bcaebc8b597d6fabc98f2340b21da67
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:44 2021 +0200

    x86/alternative: Try inline spectre_v2=retpoline,amd
    
    commit bbe2df3f6b6da7848398d55b1311d58a16ec21e4 upstream.
    
    Try and replace retpoline thunk calls with:
    
      LFENCE
      CALL    *%\reg
    
    for spectre_v2=retpoline,amd.
    
    Specifically, the sequence above is 5 bytes for the low 8 registers,
    but 6 bytes for the high 8 registers. This means that unless the
    compilers prefix stuff the call with higher registers this replacement
    will fail.
    
    Luckily GCC strongly favours RAX for the indirect calls and most (95%+
    for defconfig-x86_64) will be converted. OTOH clang strongly favours
    R11 and almost nothing gets converted.
    
    Note: it will also generate a correct replacement for the Jcc.d32
    case, except unless the compilers start to prefix stuff that, it'll
    never fit. Specifically:
    
      Jncc.d8 1f
      LFENCE
      JMP     *%\reg
    1:
    
    is 7-8 bytes long, where the original instruction in unpadded form is
    only 6 bytes.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.359986601@infradead.org
    [cascardo: RETPOLINE_AMD was renamed to RETPOLINE_LFENCE]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a40925e2efd80d22d3ba8b4477caece062764e4d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:43 2021 +0200

    x86/alternative: Handle Jcc __x86_indirect_thunk_\reg
    
    commit 2f0cbb2a8e5bbf101e9de118fc0eb168111a5e1e upstream.
    
    Handle the rare cases where the compiler (clang) does an indirect
    conditional tail-call using:
    
      Jcc __x86_indirect_thunk_\reg
    
    For the !RETPOLINE case this can be rewritten to fit the original (6
    byte) instruction like:
    
      Jncc.d8       1f
      JMP           *%\reg
      NOP
    1:
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.296470217@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 86900f95fb060ca8ad9062f1905d3ffaf3bac707
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:42 2021 +0200

    x86/alternative: Implement .retpoline_sites support
    
    commit 7508500900814d14e2e085cdc4e28142721abbdf upstream.
    
    Rewrite retpoline thunk call sites to be indirect calls for
    spectre_v2=off. This ensures spectre_v2=off is as near to a
    RETPOLINE=n build as possible.
    
    This is the replacement for objtool writing alternative entries to
    ensure the same and achieves feature-parity with the previous
    approach.
    
    One noteworthy feature is that it relies on the thunks to be in
    machine order to compute the register index.
    
    Specifically, this does not yet address the Jcc __x86_indirect_thunk_*
    calls generated by clang, a future patch will add this.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.232495794@infradead.org
    [cascardo: small conflict fixup at arch/x86/kernel/module.c]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 936c8fcb49956843af2be61423a1d0d1f91ecafa
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:41 2021 +0200

    x86/retpoline: Create a retpoline thunk array
    
    commit 1a6f74429c42a3854980359a758e222005712aee upstream.
    
    Stick all the retpolines in a single symbol and have the individual
    thunks as inner labels, this should guarantee thunk order and layout.
    
    Previously there were 16 (or rather 15 without rsp) separate symbols and
    a toolchain might reasonably expect it could displace them however it
    liked, with disregard for their relative position.
    
    However, now they're part of a larger symbol. Any change to their
    relative position would disrupt this larger _array symbol and thus not
    be sound.
    
    This is the same reasoning used for data symbols. On their own there
    is no guarantee about their relative position wrt to one aonther, but
    we're still able to do arrays because an array as a whole is a single
    larger symbol.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.169659320@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 90ca76c83044cb49a50b31a171a8daa0adcc2c2f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:40 2021 +0200

    x86/retpoline: Move the retpoline thunk declarations to nospec-branch.h
    
    commit 6fda8a38865607db739be3e567a2387376222dbd upstream.
    
    Because it makes no sense to split the retpoline gunk over multiple
    headers.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.106290934@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6a6fcb256301a3f2ca29a1c2adda71c6ab272ab7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:39 2021 +0200

    x86/asm: Fixup odd GEN-for-each-reg.h usage
    
    commit b6d3d9944bd7c9e8c06994ead3c9952f673f2a66 upstream.
    
    Currently GEN-for-each-reg.h usage leaves GEN defined, relying on any
    subsequent usage to start with #undef, which is rude.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120310.041792350@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 71e578e1bd74d716a5cb136dbc55594dbf4450ef
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:38 2021 +0200

    x86/asm: Fix register order
    
    commit a92ede2d584a2e070def59c7e47e6b6f6341c55c upstream.
    
    Ensure the register order is correct; this allows for easy translation
    between register number and trampoline and vice-versa.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120309.978573921@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 18576e45b12a73504659d7fd82955d2aaf99ecd5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:37 2021 +0200

    x86/retpoline: Remove unused replacement symbols
    
    commit 4fe79e710d9574a14993f8b4e16b7252da72d5e8 upstream.
    
    Now that objtool no longer creates alternatives, these replacement
    symbols are no longer needed, remove them.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120309.915051744@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 655d4097039c959910dbf671f1936237b2b801e7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 24 11:41:01 2021 +0200

    objtool: Introduce CFI hash
    
    commit 8b946cc38e063f0f7bb67789478c38f6d7d457c9 upstream.
    
    Andi reported that objtool on vmlinux.o consumes more memory than his
    system has, leading to horrific performance.
    
    This is in part because we keep a struct instruction for every
    instruction in the file in-memory. Shrink struct instruction by
    removing the CFI state (which includes full register state) from it
    and demand allocating it.
    
    Given most instructions don't actually change CFI state, there's lots
    of repetition there, so add a hash table to find previous CFI
    instances.
    
    Reduces memory consumption (and runtime) for processing an
    x86_64-allyesconfig:
    
      pre:  4:40.84 real,   143.99 user,    44.18 sys,      30624988 mem
      post: 2:14.61 real,   108.58 user,    25.04 sys,      16396184 mem
    
    Suggested-by: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20210624095147.756759107@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 503882b5aeb694dfa7cb0a3fda174fbdfb6af059
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:36 2021 +0200

    objtool,x86: Replace alternatives with .retpoline_sites
    
    commit 134ab5bd1883312d7a4b3033b05c6b5a1bb8889b upstream.
    
    Instead of writing complete alternatives, simply provide a list of all
    the retpoline thunk calls. Then the kernel is free to do with them as
    it pleases. Simpler code all-round.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120309.850007165@infradead.org
    [cascardo: fixed conflict because of missing
     8b946cc38e063f0f7bb67789478c38f6d7d457c9]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cad0e43a8c302f12dc2c1c2f78b2303b942ceb37
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:35 2021 +0200

    objtool: Shrink struct instruction
    
    commit c509331b41b7365e17396c246e8c5797bccc8074 upstream.
    
    Any one instruction can only ever call a single function, therefore
    insn->mcount_loc_node is superfluous and can use insn->call_node.
    
    This shrinks struct instruction, which is by far the most numerous
    structure objtool creates.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120309.785456706@infradead.org
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c9fd00d8e622a681cf0e39b0844e4da85364d6c4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:34 2021 +0200

    objtool: Explicitly avoid self modifying code in .altinstr_replacement
    
    commit dd003edeffa3cb87bc9862582004f405d77d7670 upstream.
    
    Assume ALTERNATIVE()s know what they're doing and do not change, or
    cause to change, instructions in .altinstr_replacement sections.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120309.722511775@infradead.org
    [cascardo: context adjustment]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 29e6b52efca13369e8ce35abeabbcb105554c0f0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 26 14:01:33 2021 +0200

    objtool: Classify symbols
    
    commit 1739c66eb7bd5f27f1b69a5a26e10e8327d1e136 upstream.
    
    In order to avoid calling str*cmp() on symbol names, over and over, do
    them all once upfront and store the result.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/r/20211026120309.658539311@infradead.org
    [cascardo: no pv_target on struct symbol, because of missing
     db2b0c5d7b6f19b3c2cab08c531b65342eb5252b]
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1fd333eb3377361dfeb3c6fed577393f0a7e2c74
Author: Lai Jiangshan <jiangshan.ljs@antgroup.com>
Date:   Tue May 3 11:21:06 2022 +0800

    x86/entry: Don't call error_entry() for XENPV
    
    commit 64cbd0acb58203fb769ed2f4eab526d43e243847 upstream.
    
    XENPV guests enter already on the task stack and they can't fault for
    native_iret() nor native_load_gs_index() since they use their own pvop
    for IRET and load_gs_index(). A CR3 switch is not needed either.
    
    So there is no reason to call error_entry() in XENPV.
    
      [ bp: Massage commit message. ]
    
    Signed-off-by: Lai Jiangshan <jiangshan.ljs@antgroup.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Link: https://lore.kernel.org/r/20220503032107.680190-6-jiangshanlai@gmail.com
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cac414684be9adb6b8bb7e271b66d4ffd589769f
Author: Lai Jiangshan <jiangshan.ljs@antgroup.com>
Date:   Thu Apr 21 22:10:50 2022 +0800

    x86/entry: Move PUSH_AND_CLEAR_REGS out of error_entry()
    
    commit ee774dac0da1543376a69fd90840af6aa86879b3 upstream.
    
    The macro idtentry() (through idtentry_body()) calls error_entry()
    unconditionally even on XENPV. But XENPV needs to only push and clear
    regs.
    
    PUSH_AND_CLEAR_REGS in error_entry() makes the stack not return to its
    original place when the function returns, which means it is not possible
    to convert it to a C function.
    
    Carve out PUSH_AND_CLEAR_REGS out of error_entry() and into a separate
    function and call it before error_entry() in order to avoid calling
    error_entry() on XENPV.
    
    It will also allow for error_entry() to be converted to C code that can
    use inlined sync_regs() and save a function call.
    
      [ bp: Massage commit message. ]
    
    Signed-off-by: Lai Jiangshan <jiangshan.ljs@antgroup.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Link: https://lore.kernel.org/r/20220503032107.680190-4-jiangshanlai@gmail.com
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 19ac6c99a17e879d2f43f7650bf824f5fbfcfa0a
Author: Lai Jiangshan <jiangshan.ljs@antgroup.com>
Date:   Thu Apr 21 22:10:49 2022 +0800

    x86/entry: Switch the stack after error_entry() returns
    
    commit 520a7e80c96d655fbe4650d9cc985bd9d0443389 upstream.
    
    error_entry() calls fixup_bad_iret() before sync_regs() if it is a fault
    from a bad IRET, to copy pt_regs to the kernel stack. It switches to the
    kernel stack directly after sync_regs().
    
    But error_entry() itself is also a function call, so it has to stash
    the address it is going to return to, in %r12 which is unnecessarily
    complicated.
    
    Move the stack switching after error_entry() and get rid of the need to
    handle the return address.
    
      [ bp: Massage commit message. ]
    
    Signed-off-by: Lai Jiangshan <jiangshan.ljs@antgroup.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20220503032107.680190-3-jiangshanlai@gmail.com
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 01bc8bd64c19a26940a8a68b7df1700cd6403778
Author: Lai Jiangshan <jiangshan.ljs@antgroup.com>
Date:   Thu Apr 21 22:10:48 2022 +0800

    x86/traps: Use pt_regs directly in fixup_bad_iret()
    
    commit 0aca53c6b522f8d6e2681ca875acbbe105f5fdcf upstream.
    
    Always stash the address error_entry() is going to return to, in %r12
    and get rid of the void *error_entry_ret; slot in struct bad_iret_stack
    which was supposed to account for it and pt_regs pushed on the stack.
    
    After this, both fixup_bad_iret() and sync_regs() can work on a struct
    pt_regs pointer directly.
    
      [ bp: Rewrite commit message, touch ups. ]
    
    Signed-off-by: Lai Jiangshan <jiangshan.ljs@antgroup.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20220503032107.680190-2-jiangshanlai@gmail.com
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
